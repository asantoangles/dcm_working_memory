{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OVJdH0AWvBKw",
        "wGG5k1rWqGez"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asantoangles/dcm_working_memory/blob/main/CCN_2025_Working_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Digital brain models for working memory"
      ],
      "metadata": {
        "id": "XlIFSivW8by2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "tOUmA7GBKpFM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzJMxESH9dIC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to import required modules\n",
        "\n",
        "!pip install networkx --quiet\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.signal as sig\n",
        "import scipy.stats as stat\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import gdown\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Image\n",
        "import pandas as pd\n",
        "from google.colab import files, data_table\n",
        "import math\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.integrate import solve_ivp\n",
        "from tqdm import tqdm\n",
        "from difflib import get_close_matches\n",
        "\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_structural_data():\n",
        "\n",
        "  #########\n",
        "  ## FLN ##\n",
        "  #########\n",
        "\n",
        "  # Define the remote file to retrieve\n",
        "  remote_url = 'https://drive.google.com/uc?id=1J0V1Hig_JE2XhsPCpAV3UlYzRq8NkEl3'\n",
        "  # Define the local filename to save data\n",
        "  local_file = 'FLN.npy'\n",
        "  # Make http request for remote file data\n",
        "  gdown.download(remote_url, local_file, quiet = True)\n",
        "\n",
        "  FLN = np.load(local_file)\n",
        "\n",
        "\n",
        "\n",
        "  #########\n",
        "  ## SLN ##\n",
        "  #########\n",
        "\n",
        "  # Define the remote file to retrieve\n",
        "  remote_url = 'https://drive.google.com/uc?id=19mBa4Yv2XsfwQw3n_0EeZX8as3CFO0Co'\n",
        "  # Define the local filename to save data\n",
        "  local_file = 'SLN.npy'\n",
        "  # Make http request for remote file data\n",
        "  gdown.download(remote_url, local_file, quiet = True)\n",
        "\n",
        "  SLN = np.load(local_file)\n",
        "\n",
        "\n",
        "\n",
        "  ##################\n",
        "  ## Area centers ##\n",
        "  ##################\n",
        "\n",
        "  # Define the remote file to retrieve\n",
        "  remote_url = 'https://drive.google.com/uc?id=1amlbDasESV-IcunPRD3uWh8hjMYS-X7K'\n",
        "  # Define the local filename to save data\n",
        "  local_file = 'centers.npy'\n",
        "  # Make http request for remote file data\n",
        "  gdown.download(remote_url, local_file, quiet = True)\n",
        "\n",
        "  centers = np.load(local_file)\n",
        "\n",
        "\n",
        "\n",
        "  ##########################\n",
        "  ## Area labels and info ##\n",
        "  ##########################\n",
        "\n",
        "  # Define the remote file to retrieve\n",
        "  remote_url = 'https://drive.google.com/uc?id=13WjXAnOYNFnVcFQh8aEiyHdsxp4f2PV-'\n",
        "  # Define the local filename to save data\n",
        "  local_file = 'area_info.csv'\n",
        "  # Make http request for remote file data\n",
        "  gdown.download(remote_url, local_file, quiet = True)\n",
        "\n",
        "  # Gets list of used areas\n",
        "  area_description = pd.read_csv(local_file, delimiter=';')\n",
        "\n",
        "  ###############\n",
        "  ## Hierarchy ##\n",
        "  ###############\n",
        "\n",
        "  # Define the remote file to retrieve\n",
        "  remote_url = 'https://drive.google.com/uc?id=1SOaxTOrWxQeou8fUFFsHuq7RYaiuP1Ci'\n",
        "  # Define the local filename to save data\n",
        "  local_file = 'hierarchy.npy'\n",
        "  # Make http request for remote file data\n",
        "  gdown.download(remote_url, local_file, quiet = True)\n",
        "\n",
        "  # Gets list of used areas\n",
        "  h = np.load(local_file)\n",
        "\n",
        "  return FLN, SLN, centers, area_description, h\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plotting functions for structural matrices\n",
        "def plot_matrix(matrix, coordinates, node_sizes = 1, node_labels = None, node_label_color = 'w', node_colors = 0, node_cmap = 'gray', node_alpha = 0.5, threshold = 0.1, title = '', view = 'yz', vmin = None, vmax = None, dpi = 150):\n",
        "\n",
        "    view_dict = {'xy': (0, 1), 'xz': (0, 2),'yz': (1, 2)}\n",
        "    idx1, idx2 = view_dict[view]\n",
        "\n",
        "    node_sizes *= np.ones(matrix.shape[0])\n",
        "    node_colors *= np.ones(matrix.shape[0])\n",
        "\n",
        "\n",
        "    pos = {idx: coordinates[idx, np.array([idx1, idx2])] for idx in range(matrix.shape[0])}\n",
        "\n",
        "    if view == 'yz':\n",
        "        _, ax = plt.subplots(1, 1, figsize = (10, 6), dpi = dpi)\n",
        "    elif view == 'xy':\n",
        "        _, ax = plt.subplots(1, 1, figsize = (5, 10), dpi = dpi)\n",
        "    elif view == 'xz':\n",
        "        _, ax = plt.subplots(1, 1, figsize = (5, 6), dpi = dpi)\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "    aux = matrix.copy()\n",
        "    th = np.sort(matrix.flatten())[int((1-threshold) * len(matrix.flatten()))] # Removes X% weakest connections\n",
        "    aux[matrix < th] = 0\n",
        "    G = nx.from_numpy_array(aux.T, create_using=nx.DiGraph)\n",
        "    widths = [5 * G.get_edge_data(e[0], e[1])[\"weight\"] for e in G.edges]\n",
        "\n",
        "\n",
        "    if vmax is None or vmin is None:\n",
        "        if np.sum(node_colors < 0) > 0:\n",
        "            vmin = -np.nanmax(np.abs(node_colors))\n",
        "            vmax = np.nanmax(np.abs(node_colors))\n",
        "        else:\n",
        "            vmin = np.nanmin(node_colors)\n",
        "            vmax = np.nanmax(node_colors)\n",
        "\n",
        "    nodes = nx.draw_networkx_nodes(G, pos, node_size = 300 * node_sizes, node_color=node_colors, cmap = node_cmap,\n",
        "                                   vmin = vmin, vmax = vmax, ax = ax, alpha = node_alpha)\n",
        "    edges = nx.draw_networkx_edges(G, pos, node_size = 300 * node_sizes, ax = ax, arrows = True,\n",
        "                                arrowstyle=\"-|>\", arrowsize=10, edge_color='k', width=widths,\n",
        "                                connectionstyle=\"arc3,rad=0.15\")\n",
        "\n",
        "    if node_labels is not None:\n",
        "        for i in range(len(node_labels)):\n",
        "            ax.text(coordinates[i, idx1], coordinates[i, idx2], node_labels[i], ha = 'center', va = 'center', fontsize = 5, color = node_label_color)\n",
        "\n",
        "\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "    if view == 'yz':\n",
        "        ax.set_xlabel('Posterior                                                                                                                                                                                          Anterior')\n",
        "        ax.set_ylabel('Ventral                                                                                                      Dorsal')\n",
        "\n",
        "    if view == 'xy':\n",
        "        ax.set_xlabel('Lateral                                                                              Medial')\n",
        "        ax.set_ylabel('Posterior                                                                                                                                                                                          Anterior')\n",
        "\n",
        "    if view == 'xz':\n",
        "        ax.set_xlabel('Lateral                                                                              Medial')\n",
        "        ax.set_ylabel('Ventral                                                                                                      Dorsal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_matrix_EI(matrix, coordinates, node_sizes = 1, node_labels = None, node_label_color = 'w', node_colors = 0, node_cmap = 'gray', node_alpha = 0.5, threshold = 0.1, title = '', view = 'yz', vmin = None, vmax = None, dpi = 150):\n",
        "\n",
        "    view_dict = {'xy': (0, 1), 'xz': (0, 2),'yz': (1, 2)}\n",
        "    idx1, idx2 = view_dict[view]\n",
        "\n",
        "    node_sizes *= np.ones(matrix.shape[0])\n",
        "    node_colors *= np.ones(matrix.shape[0])\n",
        "\n",
        "\n",
        "    pos = {idx: coordinates[idx, np.array([idx1, idx2])] for idx in range(matrix.shape[0])}\n",
        "\n",
        "    if view == 'yz':\n",
        "        _, ax = plt.subplots(1, 1, figsize = (10, 6), dpi = dpi)\n",
        "    elif view == 'xy':\n",
        "        _, ax = plt.subplots(1, 1, figsize = (5, 10), dpi = dpi)\n",
        "    elif view == 'xz':\n",
        "        _, ax = plt.subplots(1, 1, figsize = (5, 6), dpi = dpi)\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "    aux = matrix.copy()\n",
        "    th = np.sort(np.abs(matrix).flatten())[int((1-threshold) * len(matrix.flatten()))] # Removes X% weakest connections\n",
        "    aux[np.abs(matrix) < th] = 0\n",
        "    G = nx.from_numpy_array(aux.T, create_using=nx.DiGraph)\n",
        "    widths = [5 * G.get_edge_data(e[0], e[1])[\"weight\"] for e in G.edges]\n",
        "\n",
        "    if vmax is None or vmin is None:\n",
        "        if np.sum(node_colors < 0) > 0:\n",
        "            vmin = -np.nanmax(np.abs(node_colors))\n",
        "            vmax = np.nanmax(np.abs(node_colors))\n",
        "        else:\n",
        "            vmin = np.nanmin(node_colors)\n",
        "            vmax = np.nanmax(node_colors)\n",
        "\n",
        "    nodes = nx.draw_networkx_nodes(G, pos, node_size = 300 * node_sizes, node_color=node_colors, cmap = node_cmap,\n",
        "                                   vmin = vmin, vmax = vmax, ax = ax, alpha = node_alpha)\n",
        "\n",
        "\n",
        "    # Plots excitation dominated edges\n",
        "    aux_E = aux.copy()\n",
        "    aux_E[aux_E < 0] = 0\n",
        "    G = nx.from_numpy_array(aux_E.T, create_using=nx.DiGraph)\n",
        "    widths = [15 * G.get_edge_data(e[0], e[1])[\"weight\"] for e in G.edges]\n",
        "\n",
        "    edges = nx.draw_networkx_edges(G, pos, node_size=300 * node_sizes, ax = ax, arrows = True,\n",
        "                                arrowstyle=\"-|>\", arrowsize=10, edge_color='darkred', width=widths,\n",
        "                                connectionstyle=\"arc3,rad=0.15\")\n",
        "\n",
        "    # Plots inhibition dominated edges\n",
        "    aux_I = aux.copy()\n",
        "    aux_I[aux_I > 0] = 0\n",
        "    G = nx.from_numpy_array(aux_I.T, create_using=nx.DiGraph)\n",
        "    widths = [15 * G.get_edge_data(e[0], e[1])[\"weight\"] for e in G.edges]\n",
        "\n",
        "    edges = nx.draw_networkx_edges(G, pos, node_size=300 * node_sizes, ax = ax, arrows = True,\n",
        "                            arrowstyle=\"-|>\", arrowsize=10, edge_color='teal', width=widths,\n",
        "                            connectionstyle=\"arc3,rad=0.15\")\n",
        "\n",
        "    if node_labels is not None:\n",
        "        for i in range(len(node_labels)):\n",
        "            ax.text(coordinates[i, idx1], coordinates[i, idx2], node_labels[i], ha = 'center', va = 'center', fontsize = 5, color = node_label_color)\n",
        "\n",
        "\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "    if view == 'yz':\n",
        "        ax.set_xlabel('Posterior                                                                                                                                                                                          Anterior')\n",
        "        ax.set_ylabel('Ventral                                                                                                      Dorsal')\n",
        "\n",
        "    if view == 'xy':\n",
        "        ax.set_xlabel('Lateral                                                                              Medial')\n",
        "        ax.set_ylabel('Posterior                                                                                                                                                                                          Anterior')\n",
        "\n",
        "    if view == 'xz':\n",
        "        ax.set_xlabel('Lateral                                                                              Medial')\n",
        "        ax.set_ylabel('Ventral                                                                                                      Dorsal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def dxdt(t, x, tau_noise, sigma):\n",
        "    \"\"\"\n",
        "    Compute the derivative of the OU process.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    t : float\n",
        "        Time\n",
        "    x : np.ndarray\n",
        "        1D array of length N\n",
        "    tau_noise : float\n",
        "        Time constant of the noise\n",
        "    sigma : float\n",
        "        Standard deviation of the noise term\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dxdt : np.ndarray\n",
        "        1D array of length N\n",
        "    \"\"\"\n",
        "    return -x / tau_noise + sigma * np.random.normal(0, 1, len(x)) / np.sqrt(tau_noise)\n",
        "\n",
        "\n",
        "def generate_noise_function(N, t_eval, tau_noise, sigma):\n",
        "    \"\"\"\n",
        "    Generate a noise function for a fixed sigma and return an interpolated function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N : int\n",
        "        Number of brain areas\n",
        "    t_eval : np.ndarray\n",
        "        Time points at which to save the noise (1D array)\n",
        "    tau_noise : float\n",
        "        Time constant of the noise\n",
        "    sigma : float\n",
        "        Standard deviation of the noise term\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    interp_func : callable\n",
        "        A function that takes time t and returns noise values\n",
        "    \"\"\"\n",
        "    # Generate OU noise using solve_ivp\n",
        "    sol = solve_ivp(\n",
        "        dxdt,\n",
        "        [t_eval[0], t_eval[-1]],\n",
        "        y0=np.zeros(N),\n",
        "        t_eval=t_eval,\n",
        "        args=(tau_noise, sigma),\n",
        "    )\n",
        "    noise = sol.y.T # Shape: (N_t, N)\n",
        "\n",
        "    # Create an interpolation function for the noise\n",
        "    interp_func = interp1d(\n",
        "        t_eval, noise, axis=0, kind=\"linear\", fill_value=\"extrapolate\"\n",
        "    )\n",
        "\n",
        "    return interp_func\n",
        "\n",
        "\n",
        "def correct_spine_counts(spine_counts):\n",
        "    \"\"\"\n",
        "    Correct the spine counts based on age factors. For ages 5 and 10, the spine counts are increased by 15% and 30%, respectively.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spine_counts : np.ndarray\n",
        "        Spine counts\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    corrected_spine_counts : np.ndarray\n",
        "    \"\"\"\n",
        "    # age factor\n",
        "    AF = np.ones(30)\n",
        "    AF[[13, 14, 15, 16, 18, 29]] = np.multiply(AF[[13, 14, 15, 16, 18, 29]], 1.15) # 5yo, 15% increase\n",
        "    AF[[5, 7, 20]] = np.multiply(AF[[5, 7, 20]], 1.30) # 10yo, 30% increase\n",
        "\n",
        "    return spine_counts * AF\n",
        "\n",
        "def load_and_preprocess_data(data_folder_path=None):\n",
        "    \"\"\"\n",
        "    Load and preprocess the data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_folder_path : str, optional\n",
        "        Path to the data folder containing the data files. If None, the relative path is used.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    area_names : np.ndarray\n",
        "        Array of area names\n",
        "    area_lobes : np.ndarray\n",
        "        Array of area lobes\n",
        "    area_descriptions : np.ndarray\n",
        "        Array of area descriptions\n",
        "    SLN : np.ndarray\n",
        "        Structural connectivity matrix\n",
        "    W : np.ndarray\n",
        "        Weight matrix. W_ij = 1.2 * FLN_ij^0.3 / sum_j(1.2 * FLN_ij^0.3)\n",
        "    hier_vals : np.ndarray\n",
        "        Hierarchy values. If spine counts are negative (missing), the original hierarchy values are used.\n",
        "    \"\"\"\n",
        "    if data_folder_path is None:\n",
        "        data_folder_path = \"data\"\n",
        "\n",
        "    ## Area names ##\n",
        "    export_url = 'https://drive.google.com/uc?export=download&id=1vVc1GXFb-a-J5uiu8__5enbiQUZCMNhU'\n",
        "    # Define the local filename to save data\n",
        "    local_file = 'areaList.xlsx'\n",
        "    # Make http request for remote file data\n",
        "    gdown.download(export_url, local_file, quiet = True)\n",
        "    df = pd.read_excel(local_file, header = 0)\n",
        "    area_names = df['Name'].to_numpy()\n",
        "    area_lobes = df['Lobe'].to_numpy()\n",
        "    area_descriptions = df['Description'].to_numpy()\n",
        "\n",
        "    ## SLN ##\n",
        "    export_url = 'https://drive.google.com/uc?export=download&id=1I5iebwYaYi2F7Zo3oZd_9mtQmctGgmEc'\n",
        "    local_file = 'sln.xlsx'\n",
        "    gdown.download(export_url, local_file, quiet = True)\n",
        "    df = pd.read_excel(local_file, header=None)\n",
        "    SLN = df.to_numpy()\n",
        "\n",
        "    ## FLN ##\n",
        "    export_url = 'https://drive.google.com/uc?export=download&id=1bBsSd-nWWb80fcGEB4L-yeDjgTx2OT3X'\n",
        "    local_file = 'fln.xlsx'\n",
        "    gdown.download(export_url, local_file, quiet = True)\n",
        "    df = pd.read_excel(local_file, header=None)\n",
        "    FLN = df.to_numpy()\n",
        "    W = 1.2 * FLN ** 0.3\n",
        "    W /= np.sum(W, axis=1, keepdims=True)\n",
        "\n",
        "    ## Hierarchy values ##\n",
        "    export_url = 'https://drive.google.com/uc?export=download&id=1zFV1BH8erMqVR9zQBHSP8hWslwwvcl5p'\n",
        "    local_file = 'hierVals.xlsx'\n",
        "    gdown.download(export_url, local_file, quiet = True)\n",
        "    df = pd.read_excel(local_file, header=None)\n",
        "    hier_vals1 = df.to_numpy().flatten() # hier_vals1 is the original hierarchy values\n",
        "\n",
        "    ## Spine counts ##\n",
        "    export_url = 'https://drive.google.com/uc?export=download&id=1QhW4Y9yoOzLsjJnFcKm-KH_N9unbJnMS'\n",
        "    local_file = 'spineCounts_ageCorrected.xlsx'\n",
        "    gdown.download(export_url, local_file, quiet = True)\n",
        "    df = pd.read_excel(local_file, header=None)\n",
        "    spinec = df.to_numpy().flatten()\n",
        "\n",
        "    hier_vals = spinec / np.max(spinec)\n",
        "    indices = np.where(spinec < 0)\n",
        "    hier_vals[indices] = hier_vals1[indices] # replace empty spine counts with original hierarchy values\n",
        "\n",
        "    return {'area_names': area_names, 'area_lobes': area_lobes, 'area_descriptions': area_descriptions, 'SLN': SLN, 'W': W, 'hier_vals': hier_vals}\n",
        "\n",
        "\n",
        "def get_success_rate(desc, WM_network, t_end, dt, I_ext_strengths, ts_ext_start, ts_ext_end, runs=50, threshold=10, monitor_areas_idx=range(12, 30)):\n",
        "    \"\"\"\n",
        "    Get the success rates of the WM model for a given year.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    desc : str\n",
        "        Description of the simulation\n",
        "    WM_network : WorkingMemoryNetwork\n",
        "        Working Memory network\n",
        "    t_end : float\n",
        "        End time of the simulation\n",
        "    dt : float\n",
        "        Time step\n",
        "    I_ext_strengths : np.ndarray\n",
        "        External input strengths\n",
        "    ts_ext_start : np.ndarray\n",
        "        Start times of the external inputs\n",
        "    ts_ext_end : np.ndarray\n",
        "        End times of the external inputs\n",
        "    runs : int, optional\n",
        "        Number of runs\n",
        "    threshold : float, optional\n",
        "        Threshold for distinguishing successful trials\n",
        "    monitor_areas_idx : np.ndarray, optional\n",
        "        Indices of areas to monitor and check for success\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    success_rate : float\n",
        "        Success rate\n",
        "    \"\"\"\n",
        "    success_times = 0\n",
        "    for seed in tqdm(range(runs), desc=desc):\n",
        "        WM_network.reset(random_seed=seed)\n",
        "        state_history = WM_network.run(t_end=t_end, dt=dt, I_ext_strengths=I_ext_strengths, ts_ext_start=ts_ext_start, ts_ext_end=ts_ext_end)\n",
        "        final_r_A = np.mean(state_history[int(-1/dt), monitor_areas_idx, 0], axis=0)    # mean r_A in the last one second for each area\n",
        "        if np.sum(final_r_A > threshold) > len(monitor_areas_idx) / 2:\n",
        "            success_times += 1\n",
        "    return success_times / runs"
      ],
      "metadata": {
        "id": "Vp12WXvhKyNk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "class WorkingMemoryParameters:\n",
        "    \"\"\"\n",
        "    This class includes various parameters involved in the dynamics of a working memory model.\n",
        "    Parameters are stored in a dictionary for flexibility and can be updated using the `update` method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Directly initializing self._params\n",
        "        self.__dict__[\"_params\"] = {\n",
        "            # I -> r\n",
        "            \"A\": 135,           # Hz/nA\n",
        "            \"B\": 54,            # Hz\n",
        "            \"D\": 0.308,         # s\n",
        "            \"G_I\": 4,\n",
        "            \"C_0\": 177,         # Hz\n",
        "            \"C_1\": 615,         # Hz/nA\n",
        "            \"R_0\": 5.5,         # Hz\n",
        "            \"TAU_R\": 2e-3,      # s\n",
        "\n",
        "            # r -> S\n",
        "            \"TAU_N\": 60e-3,     # s\n",
        "            \"TAU_G\": 5e-3,      # s\n",
        "            \"GAMMA\": 1.282,\n",
        "            \"GAMMA_I\": 2.0,\n",
        "\n",
        "            # S -> I\n",
        "            \"J_S\": 0.3213,      # nA\n",
        "            \"J_C\": 0.0107,      # nA\n",
        "            \"J_IE\": 0.15,       # nA\n",
        "            \"J_EI\": -0.31,      # nA\n",
        "            \"J_II\": -0.12,      # nA\n",
        "            \"I_0A\": 0.3294,     # nA\n",
        "            \"I_0B\": 0.3294,     # nA\n",
        "            \"I_0C\": 0.26,       # nA\n",
        "\n",
        "            # Noise\n",
        "            \"TAU_NOISE\": 2e-3,  # s\n",
        "            \"SIGMA_A\": 0.01,    # nA\n",
        "            \"SIGMA_B\": 0.01,    # nA\n",
        "\n",
        "            # Gradient of J_s\n",
        "            \"J_MIN\": 0.21,      # nA\n",
        "            \"J_MAX\": 0.42,      # nA (< 0.4655)\n",
        "\n",
        "            # Gradient of J_IE\n",
        "            \"J_0\": 0.2112,      # nA\n",
        "            \"ZETA\": None,\n",
        "\n",
        "            # Inter-areal projections\n",
        "            \"G\": 0.48,          # global coupling strength\n",
        "            \"Z\": None,          # E-I balancing factor\n",
        "            \"ALPHA\": 1          # relative strength of (feedback) inhibitory projections\n",
        "        }\n",
        "\n",
        "        # Initialize dependent parameters\n",
        "        self._recalculate_dependent_parameters()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        \"\"\"\n",
        "        Dynamically retrieve the value of a parameter.\n",
        "        \"\"\"\n",
        "        # Safely access self._params using __dict__ to avoid recursion\n",
        "        params = self.__dict__.get(\"_params\", {})\n",
        "        if name in params:\n",
        "            return params[name]\n",
        "\n",
        "        # Suggest similar parameter names\n",
        "        similar_keys = get_close_matches(name, params.keys(), n=3, cutoff=0.5)\n",
        "        if similar_keys:\n",
        "            suggestion = f\" Did you mean one of these? {similar_keys}\"\n",
        "        else:\n",
        "            suggestion = f\" Available parameters are: {list(params.keys())}\"\n",
        "\n",
        "        raise AttributeError(\n",
        "            f\"'WorkingMemoryParameters' object has no attribute '{name}'.\"\n",
        "            f\"{suggestion}\"\n",
        "        )\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Update parameters of the model.\n",
        "        \"\"\"\n",
        "        dependent_params = [\"ZETA\", \"Z\"]\n",
        "        for key, value in kwargs.items():\n",
        "            if key in dependent_params:\n",
        "                raise AttributeError(f\"Cannot directly update dependent parameter '{key}'.\")\n",
        "            if key.upper() in map(str.upper, self._params.keys()):  # Ignore case\n",
        "                matching_key = [k for k in self._params.keys() if k.upper() == key.upper()][0]\n",
        "                self._params[matching_key] = value\n",
        "            else:\n",
        "                raise AttributeError(f\"Invalid parameter '{key}'. Available parameters are: {list(self._params.keys())}\")\n",
        "        self._recalculate_dependent_parameters()\n",
        "\n",
        "    def _recalculate_dependent_parameters(self):\n",
        "        \"\"\"\n",
        "        Recalculate dependent parameters after updating.\n",
        "        \"\"\"\n",
        "        self._params[\"ZETA\"] = self.calc_zeta()\n",
        "        self._params[\"Z\"] = self.calc_Z()\n",
        "\n",
        "    def calc_zeta(self):\n",
        "        \"\"\"\n",
        "        Compute the inhibitory-to-excitatory balancing factor (ZETA).\n",
        "        \"\"\"\n",
        "        tau_g = self._params[\"TAU_G\"]\n",
        "        gamma_i = self._params[\"GAMMA_I\"]\n",
        "        c_1 = self._params[\"C_1\"]\n",
        "        j_ii = self._params[\"J_II\"]\n",
        "        g_i = self._params[\"G_I\"]\n",
        "\n",
        "        denominator = g_i - j_ii * tau_g * gamma_i * c_1\n",
        "        if denominator <= 0:\n",
        "            raise ValueError(\"Denominator in ZETA calculation is non-positive.\")\n",
        "        return (tau_g * gamma_i * c_1) / denominator\n",
        "\n",
        "    def calc_Z(self):\n",
        "        \"\"\"\n",
        "        Compute the E-I balancing factor (Z).\n",
        "        \"\"\"\n",
        "        tau_g = self._params[\"TAU_G\"]\n",
        "        gamma_i = self._params[\"GAMMA_I\"]\n",
        "        c_1 = self._params[\"C_1\"]\n",
        "        j_ei = self._params[\"J_EI\"]\n",
        "        j_ii = self._params[\"J_II\"]\n",
        "        g_i = self._params[\"G_I\"]\n",
        "\n",
        "        denominator = c_1 * tau_g * gamma_i * j_ii - g_i\n",
        "        if denominator == 0:\n",
        "            raise ValueError(\"Denominator in Z calculation is zero.\")\n",
        "        return (2 * c_1 * tau_g * gamma_i * j_ei) / denominator\n",
        "params = WorkingMemoryParameters()"
      ],
      "metadata": {
        "id": "DACi3-wdpl6G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Working memory class\n",
        "class WorkingMemoryNetwork:\n",
        "    def __init__(self, area_names, area_lobes, Y0, W, F, h, params, remove_FB=False, random_seed=None):\n",
        "        \"\"\"\n",
        "        Initialize the network object for the working memory model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        area_names: list\n",
        "            List of brain area names.\n",
        "        area_lobes: list\n",
        "            List of brain area lobes.\n",
        "        Y0: np.ndarray\n",
        "            N x 8 matrix of initial conditions. Each row is a brain area, each column is a variable vector (r_A, r_B, r_C, S_A, S_B, S_C, x_A, x_B).\n",
        "        W: np.ndarray\n",
        "            N x N matrix of connectivity strengths.\n",
        "        F: np.ndarray\n",
        "            N x N matrix of feedforward (FF) relative strengths.\n",
        "        h: np.ndarray\n",
        "            1D array of the positions of the brain areas in the T1w:T2w ranking.\n",
        "        params: object\n",
        "            Parameters object containing the parameters of the model.\n",
        "        remove_FB: bool, optional\n",
        "            Whether to remove feedback projections. Default is False.\n",
        "        random_seed: int, optional\n",
        "            Random seed for reproducibility. Default is None.\n",
        "        \"\"\"\n",
        "        self.area_names = area_names.copy()\n",
        "        self.area_lobes = area_lobes.copy()\n",
        "        self.N = len(area_names) # number of brain areas\n",
        "\n",
        "        # Check the dimensions of the input matrices\n",
        "        assert len(area_lobes) == self.N, f\"Length of area_lobes must be {self.N}!\"\n",
        "        assert Y0.shape == (self.N, 8), f\"Y0 must be a {self.N} x 8 matrix!\"\n",
        "        assert W.shape == (self.N, self.N), f\"W must be a {self.N} x {self.N} matrix!\"\n",
        "        assert F.shape == (self.N, self.N), f\"F must be a {self.N} x {self.N} matrix!\"\n",
        "        assert self.N == h.shape[0], f\"The length of h must be {self.N}!\"\n",
        "        assert np.min(W) >= 0, \"Ensure all connections are non-negative!\"\n",
        "        assert np.all(np.diag(W) == 0), \"Ensure no self-connections!\"\n",
        "        assert np.min(F) >= 0 and np.max(F) <= 1, \"Ensure all feedforward relative strengths are between 0 and 1!\"\n",
        "        assert np.min(h) >= 0 and np.max(h) <= 1, \"Ensure all hierarchical level values are between 0 and 1!\"\n",
        "\n",
        "        self.Y0 = Y0.copy() # store initial conditions\n",
        "        self.Y = Y0.copy() # initialize current state\n",
        "        self.W = W.copy() # connectivity matrix (FLN)\n",
        "        self.F = F.copy() # FF relative strength matrix (SLN)\n",
        "        self.h = h.copy() # hierarchical level\n",
        "        self.params = params\n",
        "\n",
        "        self.J_s = self.calc_J_s() # calculate gradient of J_s\n",
        "        self.J_IE = self.calc_J_IE() # calculate gradient of J_IE\n",
        "\n",
        "        # \"SLN-driven modulation of FB projections between frontal areas is not too large, so that interactions between these areas are never strongly inhibitory.\n",
        "        # In practice, such constraint is only necessary for projections from frontal areas to 8 l and 8 m.\n",
        "        # We consider that the SLN-driven modulation of FB projections to 8 l and 8 m is never larger than 0.4.\"\n",
        "        self.lambda_I = 1 - self.F # relative strength of (feedback) inhibitory projections\n",
        "        self.frontal_indices = [i for i in range(self.N) if self.area_lobes[i] in ('Frontal', 'Prefrontal')]\n",
        "        inhib_max = 0.4 # maximum inhibitory strength\n",
        "        for i in range(2): # 8l and 8m are the first two frontal areas\n",
        "            for source_area_idx in self.frontal_indices:\n",
        "                x_idx = self.frontal_indices[i]\n",
        "                y_idx = source_area_idx\n",
        "                self.lambda_I[x_idx, y_idx] = min(self.lambda_I[x_idx, y_idx], inhib_max)\n",
        "\n",
        "        self.W_E, self.W_I = self.calc_WE_WI()\n",
        "\n",
        "        if remove_FB: # remove feedback projections\n",
        "            self.W_E -= np.triu(self.W_E, k=1)\n",
        "            self.W_I -= np.triu(self.W_I, k=1)\n",
        "\n",
        "        self.state_histories = [] # list to store the state history\n",
        "        self.time_histories = [] # list to store the time history\n",
        "        self.monitor = [] # list to store the monitored variables\n",
        "\n",
        "        if random_seed:\n",
        "            self.random_seed = random_seed\n",
        "            self.rng = np.random.default_rng(random_seed)\n",
        "        else:\n",
        "            self.random_seed = None\n",
        "            self.rng = np.random.default_rng()\n",
        "\n",
        "    def calc_WE_WI(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        W_E, W_I - strenghts of excitatory - excitatory and excitatory - inhibitory long range connection strength, respectively.\n",
        "        \"\"\"\n",
        "        # Below, we denote SLN as self.F and (1 - SLN) as self.lambda_I\n",
        "        # np.outer(self.J_s / np.max(self.J_s) and np.outer(self.J_IE / np.max(self.J_IE), np.ones(self.N))\n",
        "        # are normalizing factors\n",
        "        W_E = self.params.G * self.W * np.outer(self.J_s / np.max(self.J_s), np.ones(self.N)) * self.F\n",
        "        W_I = self.params.ALPHA * self.params.G / self.params.Z * self.W * np.outer(self.J_IE / np.max(self.J_IE), np.ones(self.N)) * self.lambda_I\n",
        "\n",
        "        return W_E, W_I\n",
        "\n",
        "    def calc_J_s(self):\n",
        "        \"\"\"\n",
        "        Calculate the synaptic strength J_s for each brain area.\n",
        "        \"\"\"\n",
        "        return self.params.J_MIN + (self.params.J_MAX - self.params.J_MIN) * self.h\n",
        "\n",
        "    def calc_J_IE(self):\n",
        "        \"\"\"\n",
        "        Calculate the synaptic strength J_IE for each brain area.\n",
        "        \"\"\"\n",
        "        return (self.params.J_0 - self.J_s - self.params.J_C) / (2 * self.params.J_EI * self.params.ZETA)\n",
        "\n",
        "    def phi_E(self, I):\n",
        "        \"\"\"\n",
        "        Transfer function for excitatory populations.\n",
        "        \"\"\"\n",
        "        return (self.params.A * I - self.params.B) / (1 - np.exp(-self.params.D * (self.params.A * I - self.params.B)))\n",
        "\n",
        "    def phi_I(self, I):\n",
        "        \"\"\"\n",
        "        Transfer function for inhibitory populations.\n",
        "        \"\"\"\n",
        "        return np.maximum(self.params.R_0 + (self.params.C_1 * I - self.params.C_0) / self.params.G_I, 0)\n",
        "\n",
        "    def calc_dY(self, Y, dt, dt_over_taus, I_ext):\n",
        "        \"\"\"\n",
        "        Calculate the differential of Y for this N x 8 system.\n",
        "        Function will become a method to the working memory class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Y: np.ndarray\n",
        "            N x 8 matrix of variables (r_A, r_B, r_C, S_A, S_B, S_C, x_A, x_B).\n",
        "        dt: float\n",
        "            Time step size.\n",
        "        dt_over_taus: np.ndarray\n",
        "            Precomputed array of time step ratios (dt/tau_r, dt/tau_N, dt/tau_G, dt/tau_noise, sqrt(dt/tau_noise)).\n",
        "        I_ext: np.ndarray\n",
        "            N x 3 matrix of external inputs. Each row is a brain area, each column is an external input (I_ext_A, I_ext_B, I_ext_C).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dY: np.ndarray\n",
        "            N x 8 matrix of differentials.\n",
        "        \"\"\"\n",
        "        # Extract variables and precomputed time step ratios\n",
        "        r_A, r_B, r_C = Y[:, 0], Y[:, 1], Y[:, 2]\n",
        "        S_A, S_B, S_C = Y[:, 3], Y[:, 4], Y[:, 5]\n",
        "        x_A, x_B = Y[:, 6], Y[:, 7]\n",
        "        dt_over_tau_r, dt_over_tau_N, dt_over_tau_G, dt_over_tau_noise, sqrt_dt_over_tau_noise = dt_over_taus\n",
        "\n",
        "        # Compute the inputs\n",
        "        I_A = self.J_s * S_A + self.params.J_C * S_B + self.params.J_EI * S_C + self.params.I_0A + self.W_E @ S_A + x_A + I_ext[:, 0]\n",
        "        I_B = self.params.J_C * S_A + self.J_s * S_B + self.params.J_EI * S_C + self.params.I_0B + self.W_E @ S_B + x_B + I_ext[:, 1]\n",
        "        I_C = self.J_IE * S_A + self.J_IE * S_B + self.params.J_II * S_C + self.params.I_0C + self.W_I @ (S_A + S_B) + I_ext[:, 2]\n",
        "\n",
        "        # Compute the differentials of firing rates\n",
        "        dr_A = (-r_A + self.phi_E(I_A)) * dt_over_tau_r\n",
        "        dr_B = (-r_B + self.phi_E(I_B)) * dt_over_tau_r\n",
        "        dr_C = (-r_C + self.phi_I(I_C)) * dt_over_tau_r\n",
        "\n",
        "        # Compute the differentials of synaptic conductances\n",
        "        dS_A = -S_A * dt_over_tau_N + self.params.GAMMA * (1 - S_A) * r_A * dt\n",
        "        dS_B = -S_B * dt_over_tau_N + self.params.GAMMA * (1 - S_B) * r_B * dt\n",
        "        dS_C = -S_C * dt_over_tau_G + self.params.GAMMA_I * r_C * dt\n",
        "\n",
        "        # Compute the differentials of noise terms (without instantaneous part)\n",
        "        dx_A = -x_A * dt_over_tau_noise + self.params.SIGMA_A * sqrt_dt_over_tau_noise * self.rng.normal(loc=0, scale=1, size=(self.N,))\n",
        "        dx_B = -x_B * dt_over_tau_noise + self.params.SIGMA_B * sqrt_dt_over_tau_noise * self.rng.normal(loc=0, scale=1, size=(self.N,))\n",
        "\n",
        "        # Combine the differentials\n",
        "        dY = np.zeros_like(Y)\n",
        "        dY[:, 0] = dr_A\n",
        "        dY[:, 1] = dr_B\n",
        "        dY[:, 2] = dr_C\n",
        "        dY[:, 3] = dS_A\n",
        "        dY[:, 4] = dS_B\n",
        "        dY[:, 5] = dS_C\n",
        "        dY[:, 6] = dx_A\n",
        "        dY[:, 7] = dx_B\n",
        "\n",
        "        return dY\n",
        "\n",
        "    def precompute_dt_over_taus(self, dt):\n",
        "        \"\"\"\n",
        "        Precompute time step ratios to avoid recalculating during iterations.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dt: float\n",
        "            Time step size.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dt_over_taus: np.ndarray\n",
        "            Array of time step ratios and their square root for noise calculations.\n",
        "        \"\"\"\n",
        "        taus = np.array([self.params.TAU_R, self.params.TAU_N, self.params.TAU_G, self.params.TAU_NOISE])\n",
        "        remainders = taus % dt\n",
        "        assert remainders.all() == 0, f\"dt is suggested to be divisible by all time constants: {taus}\"\n",
        "        dt_over_taus = dt / taus\n",
        "        return np.append(dt_over_taus, np.sqrt(dt_over_taus[-1]))  # Add sqrt(dt/tau_noise) for noise\n",
        "\n",
        "    def run(self, t_end, dt, t_start=0, I_ext_strengths=None, ts_ext_start=None, ts_ext_end=None, lesion_areas=None):\n",
        "        \"\"\"\n",
        "        Run the simulation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        t_end: float\n",
        "            End time of the simulation.\n",
        "        dt: float\n",
        "            Time step.\n",
        "        t_start: float, optional\n",
        "            Start time of the simulation. Default is 0.\n",
        "        I_ext_strengths: np.ndarray, optional\n",
        "            N_period x N x 3 matrix of external inputs strengths. N_period is the number of input periods. In each input period, the external input strength is a N x 3 matrix. If None, no external input.\n",
        "        ts_ext_start: np.ndarray, optional\n",
        "            Start times of the external input. If None, no external input.\n",
        "        ts_ext_end: np.ndarray, optional\n",
        "            End times of the external input. If None, no external input.\n",
        "        lesion_areas: list, optional\n",
        "            List of brain areas to lesion. If None, no lesion.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state_history: np.ndarray\n",
        "            State history in this simulation (len(t_eval) x N x 8 array).\n",
        "        \"\"\"\n",
        "        # Generate time points\n",
        "        t_eval = np.arange(t_start, t_end+dt, dt)\n",
        "        self.time_histories.append(t_eval)\n",
        "\n",
        "        # Check if external input is provided\n",
        "        if I_ext_strengths is None:\n",
        "            ts_ext_start, ts_ext_end = None, None\n",
        "        else:\n",
        "            assert I_ext_strengths.shape[0] == len(ts_ext_start) == len(ts_ext_end), \"Number of external input periods should be consistent!\"\n",
        "            assert I_ext_strengths.shape[1:] == (self.N, 3), \"In each external input period, I_ext_strength should be an N x 3 matrix!\"\n",
        "            for t_ext_start, t_ext_end in zip(ts_ext_start, ts_ext_end):\n",
        "                assert t_ext_start < t_ext_end, \"ts_ext_start must be less than ts_ext_end!\"\n",
        "                assert t_ext_start >= t_start and t_ext_end <= t_end, \"External input time must be within the simulation time!\"\n",
        "\n",
        "        # Precompute time step ratios\n",
        "        dt_over_taus = self.precompute_dt_over_taus(dt)\n",
        "\n",
        "        # Euler-Maruyama integration\n",
        "        Y = self.Y.copy()\n",
        "        state_history = [Y.copy()]\n",
        "        for i, t in enumerate(t_eval[1:], start=1):\n",
        "            # Generate I_ext\n",
        "            I_ext = np.zeros((self.N, 3))\n",
        "            if ts_ext_start is not None: # if external input is provided\n",
        "                for j, (t_ext_start, t_ext_end) in enumerate(zip(ts_ext_start, ts_ext_end)): # loop over external input periods\n",
        "                    if t_ext_start <= t < t_ext_end:\n",
        "                        I_ext += I_ext_strengths[j]\n",
        "\n",
        "            # Calculate the differentials\n",
        "            dY = self.calc_dY(Y, dt, dt_over_taus, I_ext)\n",
        "\n",
        "            # Update the state\n",
        "            Y += dY\n",
        "\n",
        "            # Apply lesion if necessary\n",
        "            if lesion_areas:\n",
        "                if t > 0:\n",
        "                    for lesion_area in lesion_areas:\n",
        "                        area_idx = np.where(np.array(self.area_names) == lesion_area)[0][0]\n",
        "                        Y[area_idx, :6] = 0\n",
        "\n",
        "            # Store the current state\n",
        "            state_history.append(Y.copy())\n",
        "\n",
        "        # Store the state history\n",
        "        self.state_histories.append(np.array(state_history))\n",
        "\n",
        "        # Update the current state\n",
        "        self.Y = Y\n",
        "\n",
        "        return np.array(state_history)\n",
        "\n",
        "    def get_state_histories(self, variables=['all']):\n",
        "        \"\"\"\n",
        "        Get the state histories of the network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        variables: list\n",
        "            list of variables to return. If ['all'], return all variables.\n",
        "        Returns\n",
        "        -------\n",
        "        state_histories: list\n",
        "            List of state matrices for all simulations (length N_sim). Each element is a N_t x N x N_var matrix.\n",
        "        \"\"\"\n",
        "        if 'all' in variables:\n",
        "            return self.state_histories\n",
        "\n",
        "        hash_map = {'r_A': 0, 'r_B': 1, 'r_C': 2, 'S_A': 3, 'S_B': 4, 'S_C': 5, 'x_A': 6, 'x_B': 7}\n",
        "        indices = [hash_map[variable] for variable in variables]\n",
        "\n",
        "        extracted_histories = []\n",
        "        for state_history in self.state_histories:\n",
        "            # state_matrix is of shape (time_steps, N, 8)\n",
        "            # Extract the desired variables along the last axis\n",
        "            extracted_history = state_history[:, :, indices]\n",
        "            extracted_histories.append(extracted_history)\n",
        "\n",
        "        return extracted_histories\n",
        "\n",
        "    def merge_histories(self):\n",
        "        \"\"\"\n",
        "        Merge the time histories and state histories of all simulations respectively.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        merged_time: np.ndarray\n",
        "            Merged time histories (length N_t_total).\n",
        "        merged_state: np.ndarray\n",
        "            Merged state histories (N_t_total x N x 8).\n",
        "        \"\"\"\n",
        "        # Check if history is available\n",
        "        if not self.time_histories or not self.state_histories:\n",
        "            raise ValueError(\"No history to merge. Run a simulation first.\")\n",
        "\n",
        "        # Initialize merged arrays\n",
        "        merged_time = self.time_histories[0]\n",
        "        merged_state = self.state_histories[0]\n",
        "\n",
        "        # Iterate over the remaining histories\n",
        "        for i in range(1, len(self.time_histories)):\n",
        "            time = self.time_histories[i]\n",
        "            state = self.state_histories[i]\n",
        "\n",
        "            # Concatenate time and state\n",
        "            merged_time = np.concatenate((merged_time, time[1:] + merged_time[-1] - time[0]))\n",
        "            merged_state = np.concatenate((merged_state, state[1:]), axis=0)\n",
        "\n",
        "        return merged_time, merged_state\n",
        "\n",
        "    def clear_histories(self):\n",
        "        \"\"\"\n",
        "        Clear the time histories and state histories.\n",
        "        \"\"\"\n",
        "        self.state_histories = []\n",
        "        self.time_histories = []\n",
        "        self.monitor = []\n",
        "\n",
        "    def reset(self, Y0=None, random_seed=None):\n",
        "        \"\"\"\n",
        "        Reset the network to a given initial state or the original initial state.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Y0: np.ndarray, optional\n",
        "            N x 8 matrix of initial conditions. Each row is a brain area, each column is a variable vector (r_A, r_B, r_C, S_A, S_B, S_C, x_A, x_B). If None, use the original initial state.\n",
        "        random_seed: int, optional\n",
        "            Random seed for reproducibility. If None, use the original random seed.\n",
        "        \"\"\"\n",
        "        self.Y0 = Y0 or self.Y0 # if Y0 is not provided, use the original initial state\n",
        "        self.Y = self.Y0 # reset the state\n",
        "        self.clear_histories()\n",
        "        if random_seed:\n",
        "            self.random_seed = random_seed\n",
        "        self.rng = np.random.default_rng(self.random_seed) # reset the random number generator\n",
        "\n",
        "    def plot_all_areas(self, xlim, ylim, axes_flat=None, variables=['r_A', 'r_B'], colors=[[.1, .6, .8], [.6, 0, .5]], title=None, save_path=None, downsample=10, legend=True):\n",
        "        \"\"\"\n",
        "        Plot the variables of all brain areas.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xlim: tuple\n",
        "            Tuple of x-axis limits (start, end).\n",
        "        axes_flat: matplotlib.axes.Axes, optional\n",
        "            Flattened axes object to plot on. If None, create a new figure.\n",
        "        variables: list of str\n",
        "            List of variables to plot. Must match variable names in state history.\n",
        "        colors: list\n",
        "            List of colors for each variable, must match the length of `variables`.\n",
        "        title: str, optional\n",
        "            Title of the plot.\n",
        "        save_path: str, optional\n",
        "            Path to save the plot. If None, do not save the plot.\n",
        "        downsample: int, optional\n",
        "            Downsampling factor for time series data to speed up plotting. Default is 10.\n",
        "        legend: bool, optional\n",
        "            Whether to show the legend. Default is True.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        lines: list\n",
        "            List of Line2D objects representing the plotted data.\n",
        "        \"\"\"\n",
        "        # Check variables and colors\n",
        "        if not isinstance(variables, list) or not all(isinstance(v, str) for v in variables):\n",
        "            raise ValueError(\"`variables` must be a list of strings representing variable names.\")\n",
        "        assert len(variables) == len(colors), \"Length of variables and colors must be the same.\"\n",
        "\n",
        "        # Prepare data and area names\n",
        "        state_history = self.get_state_histories(variables)[-1]\n",
        "        t = self.time_histories[-1] # Time array\n",
        "        t_downsampled = t[::downsample] # Downsample time\n",
        "        t_end = t[-1] # Use the last time point as t_end\n",
        "\n",
        "        # Prepare plotting\n",
        "        rows = math.ceil(self.N / 6) # Determine grid rows\n",
        "        existing_plots = False # Track if there are existing plots in the axes\n",
        "        if axes_flat is None:\n",
        "            fig, axes = plt.subplots(rows, 6, figsize=(15, 2 * rows), dpi=100)\n",
        "            fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "            axes = axes.flatten() # Flatten for easy iteration\n",
        "        else:\n",
        "            axes = axes_flat\n",
        "            if axes[0].lines:\n",
        "                existing_plots = True\n",
        "\n",
        "        # Plot firing rates for each brain area\n",
        "        lines = []\n",
        "        for i, ax in enumerate(axes[:self.N]): # Only iterate over valid brain areas\n",
        "            ymax = 0 # Track max y value for this subplot\n",
        "            for var_idx, var in enumerate(variables):\n",
        "                data = state_history[:, i, var_idx][::downsample] # Downsample data\n",
        "                if i == 0:\n",
        "                    line = ax.plot(t_downsampled - 4, data, label='$'+var+'$', color=colors[var_idx], linewidth=2)\n",
        "                    lines.append(line[0])\n",
        "                    if legend:\n",
        "                        ax.legend(loc=\"upper right\")\n",
        "                else:\n",
        "                    ax.plot(t_downsampled - 4, data, color=colors[var_idx], linewidth=2)\n",
        "                ymax = max(ymax, data.max()) # Update y max value\n",
        "\n",
        "            # Add titles, labels, and ylimits\n",
        "            ax.set_title(self.area_names[i], fontweight='bold')\n",
        "            if existing_plots: # If there are existing plots before calling this function\n",
        "                x_min1, x_max1 = ax.get_xlim()\n",
        "                x_min2, x_max2 = xlim\n",
        "                ax.set_xlim(min(x_min1, x_min2), max(x_max1, x_max2))\n",
        "                y_min1, y_max1 = ax.get_ylim()\n",
        "                y_min2, y_max2 = -max(ymax * 0.2, 0.5), max(ymax * 1.3, 2)\n",
        "                ax.set_ylim(min(y_min1, y_min2), max(y_max1, y_max2))\n",
        "            else: # If there are no existing plots before calling this function\n",
        "                ax.set_xlim(*xlim)\n",
        "                ax.set_ylim(-max(ymax * 0.2, 0.5), max(ymax * 1.3, 2))\n",
        "\n",
        "            if i % 6 == 0: # First column: add ylabel\n",
        "                ax.set_ylabel(\"Rate (sp/s)\")\n",
        "            if i >= (rows - 1) * 6: # Last row: add xlabel\n",
        "                ax.set_xlabel(\"Time (s)\")\n",
        "\n",
        "            if i < 1:\n",
        "                ax.set_ylim(0, 65)\n",
        "            else:\n",
        "                ax.set_ylim(*ylim)\n",
        "\n",
        "            ax.grid(linestyle=':', alpha=0.5)\n",
        "            for spine in ['top','right']:\n",
        "                ax.spines[spine].set_visible(False)\n",
        "            ax.xaxis.set_ticks_position('bottom')\n",
        "            ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "        # Remove empty subplots if self.N is not a multiple of 6\n",
        "        for j in range(self.N, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        # Adjust layout and add optional title\n",
        "        if title:\n",
        "            plt.suptitle(title, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the figure if a path is provided\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=1200)\n",
        "\n",
        "        if axes_flat is None:\n",
        "            plt.show()\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def plot_n_areas(self, xlim, ylim, target_area_names=['V1', 'MT', 'LIP', '24c', 'STPi', '9/46d'], axes_flat=None, variables=['r_A', 'r_B'], colors=[[.1, .6, .8], [.6, 0, .5]], title=None, save_path=None, downsample=10, legend=True):\n",
        "        \"\"\"\n",
        "        Plot the firing rates of selected brain areas.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xlim: tuple\n",
        "            Tuple of x-axis limits (start, end).\n",
        "        target_area_names: list, optional\n",
        "            List of brain area names to plot.\n",
        "        axes_flat: matplotlib.axes.Axes, optional\n",
        "            Flattened axes object to plot on. If None, create a new figure.\n",
        "        variables: list of str\n",
        "            List of variables to plot. Must match variable names in state history.\n",
        "        colors: list\n",
        "            List of colors for each variable, must match the length of `variables`.\n",
        "        title: str, optional\n",
        "            Title of the plot.\n",
        "        save_path: str, optional\n",
        "            Path to save the plot. If None, do not save the plot.\n",
        "        downsample: int, optional\n",
        "            Downsampling factor for time series data to speed up plotting. Default is 10.\n",
        "        legend: bool, optional\n",
        "            Whether to show the legend. Default is True.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        lines: list\n",
        "            List of Line2D objects representing the plotted data.\n",
        "        \"\"\"\n",
        "        # Check variables and colors\n",
        "        if not isinstance(variables, list) or not all(isinstance(v, str) for v in variables):\n",
        "            raise ValueError(\"`variables` must be a list of strings representing variable names.\")\n",
        "        assert len(variables) == len(colors), \"Length of variables and colors must be the same.\"\n",
        "\n",
        "        # Prepare data and area names\n",
        "        N_area = len(target_area_names)\n",
        "        state_history = self.get_state_histories(variables)[-1]\n",
        "        t = self.time_histories[-1] # Time array\n",
        "        t_downsampled = t[::downsample] # Downsample time\n",
        "        t_end = t[-1] # Use the last time point as t_end\n",
        "\n",
        "        # Prepare plotting\n",
        "        if N_area > 6:\n",
        "            columns = 6\n",
        "            rows = math.ceil(N_area / 6)\n",
        "            figsize = (15, 2 * rows)\n",
        "        else:\n",
        "            columns = 3\n",
        "            rows = math.ceil(N_area / 3)\n",
        "            figsize = (8, 2.5 * rows)\n",
        "\n",
        "        existing_plots = False # Track if there are existing plots in the axes\n",
        "        if axes_flat is None:\n",
        "            fig, axes = plt.subplots(rows, columns, figsize=figsize, dpi=100)\n",
        "            fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "            axes = axes.flatten() # Flatten for easy iteration\n",
        "        else:\n",
        "            axes = axes_flat\n",
        "            if axes[0].lines:\n",
        "                existing_plots = True\n",
        "\n",
        "        # Plot firing rates for each brain area\n",
        "        lines = []\n",
        "        for i, ax in enumerate(axes[:N_area]): # Only iterate over valid brain areas\n",
        "            area_idx = np.where(np.array(self.area_names) == target_area_names[i])[0][0]\n",
        "            ymax = 0 # Track max y value for this subplot\n",
        "            for var_idx, var in enumerate(variables):\n",
        "                data = state_history[:, area_idx, var_idx][::downsample] # Downsample data\n",
        "                if i == 0:\n",
        "                    line = ax.plot(t_downsampled - 4, data, label='$'+var+'$', color=colors[var_idx], linewidth=2)\n",
        "                    lines.append(line[0])\n",
        "                    if legend:\n",
        "                        ax.legend(loc=\"upper right\", fontsize=10)\n",
        "                else:\n",
        "                    ax.plot(t_downsampled - 4, data, color=colors[var_idx], linewidth=2)\n",
        "                ymax = max(ymax, data.max()) # Update y max value\n",
        "\n",
        "            # Add titles, labels, and ylimits\n",
        "            ax.set_title(target_area_names[i], fontsize=13, fontweight='bold')\n",
        "            if existing_plots: # If there are existing plots before calling this function\n",
        "                x_min1, x_max1 = ax.get_xlim()\n",
        "                x_min2, x_max2 = xlim\n",
        "                ax.set_xlim(min(x_min1, x_min2), max(x_max1, x_max2))\n",
        "                y_min1, y_max1 = ax.get_ylim()\n",
        "                y_min2, y_max2 = -max(ymax * 0.2, 0.5), max(ymax * 1.3, 2)\n",
        "                ax.set_ylim(min(y_min1, y_min2), max(y_max1, y_max2))\n",
        "            else: # If there are no existing plots before calling this function\n",
        "                ax.set_xlim(*xlim)\n",
        "                ax.set_ylim(-max(ymax * 0.2, 0.5), max(ymax * 1.3, 2))\n",
        "\n",
        "            if i < 1:\n",
        "                ax.set_ylim(0, 65)\n",
        "            else:\n",
        "                ax.set_ylim(*ylim)\n",
        "\n",
        "            if i % columns == 0: # First column: add ylabel\n",
        "                ax.set_ylabel(\"Rate (sp/s)\", fontsize=12)\n",
        "            if i >= (rows - 1) * columns: # Last row: add xlabel\n",
        "                ax.set_xlabel(\"Time (s)\", fontsize=12)\n",
        "\n",
        "        # Remove empty subplots if self.N is not a multiple of 6\n",
        "        for j in range(self.N, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        # Adjust layout and add optional title\n",
        "        if title:\n",
        "            plt.suptitle(title, fontsize=18, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the figure if a path is provided\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300)  # Set proper DPI for high-quality saving\n",
        "\n",
        "        if axes_flat is None:\n",
        "            plt.show()\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def avg_firing_rates_last_two_seconds(self, variables=['r_A']):\n",
        "        \"\"\"\n",
        "        Return the average firing rates (of pop A) over the last two seconds of the simulation for all brain areas.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        variables: list of str\n",
        "            List of variables to plot. Must match variable names in state history.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        # Check variables\n",
        "        if not isinstance(variables, list) or not all(isinstance(v, str) for v in variables):\n",
        "            raise ValueError(\"`variables` must be a list of strings representing variable names.\")\n",
        "\n",
        "        # Extract time and state histories\n",
        "        t = self.time_histories[-1] # Time array\n",
        "        state_history = self.get_state_histories(variables)[-1] # Firing rate data\n",
        "\n",
        "        # Identify the indices for the last two seconds\n",
        "        end_time = t[-1]\n",
        "        start_time = end_time - 2 # Last two seconds\n",
        "        indices = np.where((t >= start_time) & (t <= end_time))[0]\n",
        "\n",
        "        if len(indices) == 0:\n",
        "            raise ValueError(\"No data points found in the last two seconds of the simulation.\")\n",
        "\n",
        "        if variables == ['r_A']:\n",
        "            # Compute the average firing rates over the last two seconds of population A\n",
        "            avg_firing_rates = np.mean(state_history[indices, :, 0], axis=0)\n",
        "\n",
        "            return avg_firing_rates\n",
        "\n",
        "        elif variables == ['r_A', 'r_B']:\n",
        "            # Compute the average firing rates over the last two seconds of population A and B\n",
        "            avg_firing_rates_A, avg_firing_rates_B = np.mean(state_history[indices, :, 0], axis=0), np.mean(state_history[indices, :, 1], axis=0)\n",
        "\n",
        "            return avg_firing_rates_A, avg_firing_rates_B"
      ],
      "metadata": {
        "id": "pbxkfdTiVsdj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Building a cortical circuit"
      ],
      "metadata": {
        "id": "3zTMco5-ra0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Modelling single populations of neurons"
      ],
      "metadata": {
        "id": "PmSdOxipra0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Excitatory population\n",
        "We will start by simulating a single population of excitatory neurons. The dynamics of a population of neurons can be described by a differential equation such as the following:\n",
        "$$ \\tau_R \\frac{dr(t)}{dt} = -r(t) + Φ_E(I)  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)$$\n",
        "Where $r(t)$ is the firing rate of the population at time t, $τ_R$ is the time constant of the neural population and $I$ is the total synaptic input to the population, which can consist of background input, external stimuli, input from other populations and/or noise.\n",
        "\n",
        "\n",
        "The **transfer function** $Φ(I)$, determines how a neural population reacts to its input. For an excitatory population we use the following transfer function from (Abbott and Chance, 2005):\n",
        "\n",
        "$$ Φ_E(I) = \\frac{aI - b}{1-e^{-d(aI-b)}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)$$\n",
        "\n",
        "a, b and d are the constant parameters that shape the transfer function."
      ],
      "metadata": {
        "id": "AEHWd0nIra0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def phi_E(I):\n",
        "    return (params.A * I - params.B) / (1 - np.exp(-params.D * (params.A * I - params.B)))\n",
        "\n",
        "def simulate_excitatory_population(I_ext=None,I_0 = 0.33, TAU_R = 0.1, dt = 0.5e-3,T = 20):\n",
        "       time_steps = int(T / dt)\n",
        "       r = np.zeros(time_steps)\n",
        "       for t in range(1, time_steps):\n",
        "              I =  I_0 + I_ext[t]\n",
        "              dr = (-r[t-1] + phi_E(I)) * (dt / TAU_R)\n",
        "              r[t] = r[t-1] + dr\n",
        "       return r"
      ],
      "metadata": {
        "id": "LkBwMB2Sra0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.1"
      ],
      "metadata": {
        "id": "esHziTY6GNkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Now, we simulate the dynamics of an excitatory population for 10 seconds. This population recieves a time varying external input (This could for example be a visual stimulus). The total input to the population is shown on the top plot.\n",
        "\n",
        "# @markdown Play with the time constant parameter, what do you observe?\n",
        "\n",
        "# @markdown Execute this cell to enable the widget!\n",
        "params = WorkingMemoryParameters()\n",
        "def exc_exploration( tau_R = 0.02):\n",
        "  I_background = 0.8\n",
        "  dt= 0.5e-3\n",
        "  T = 10\n",
        "  time_steps = int(T / dt)\n",
        "  I_ext = np.zeros(time_steps)\n",
        "  I_ext[int(2/dt):int(4/dt)] = 0.2\n",
        "  I_ext[int(2.5/dt):int(3.5/dt)] = 0.4\n",
        "  I_ext[int(6.5/dt):int(7.5/dt)] = -0.5\n",
        "  I_ext[int(7.5/dt):int(8/dt)] = -0.8\n",
        "\n",
        "\n",
        "\n",
        "  r= simulate_excitatory_population(I_ext=I_ext,I_0= I_background, TAU_R= tau_R, dt=dt, T=T)\n",
        "\n",
        "  I0 = np.ones(time_steps)*I_background\n",
        "  plt.figure(figsize=(11, 2))\n",
        "  plt.plot(np.linspace(0, T, time_steps), I0+ I_ext, label=\"Total Input\", color=\"green\")\n",
        "  plt.ylim((-0.3,1.4))\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Input current (nA)\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(11, 2))\n",
        "  plt.plot(np.linspace(0, T, time_steps), r, label=\"r (Excitatory)\")\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Firing rate (Hz)\")\n",
        "  plt.legend()\n",
        "  plt.ylim((0,155))\n",
        "  plt.show()\n",
        "\n",
        "tau_label = widgets.Label(value='τ_R (s):')\n",
        "tau_slider = widgets.FloatSlider(value=0.02, min=0.01, max=1.0, step=0.01)\n",
        "tau_widget = widgets.HBox([tau_label, tau_slider])\n",
        "display(tau_widget)\n",
        "widgets.interactive_output(exc_exploration, {'tau_R': tau_slider})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oOsrvzAlra0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inhibitory population\n",
        "Not all the neurons in the brain are excitatory. To be able to build a realistic model of the brain, we need to be able to model inhibitory populations of neurons as well as excitatory ones. Dynamics of inhibitory neurons follows the same ODE as excitatory ones. Except that we use a different activation function:\n",
        "$$ \\tau_R \\frac{dr(t)}{dt} = -r(t) + Φ_I(I)  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)$$\n",
        " $$ Φ_I(I) = [\\frac{1}{g_I}(c_1I-c_0)+r_0]_+ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)$$\n",
        " where the $[x]_+$ notation denotes rectification at zero.  "
      ],
      "metadata": {
        "id": "xVKs5OGvra0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def phi_I(I):\n",
        "    return np.maximum(params.R_0 + (params.C_1 * I - params.C_0) / params.G_I, 0)\n",
        "\n",
        "def simulate_inhibitory_population(I_ext=None,I_0 = 0.33, TAU_R = 0.1, dt = 0.5e-3,T = 20):\n",
        "       time_steps = int(T / dt)\n",
        "       r = np.zeros(time_steps)\n",
        "       for t in range(1, time_steps):\n",
        "              I =  I_0 + I_ext[t]\n",
        "              dr = (-r[t-1] + phi_I(I)) * (dt / TAU_R)\n",
        "              r[t] = r[t-1] + dr\n",
        "       return r"
      ],
      "metadata": {
        "id": "RyLEOoDwra0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.2"
      ],
      "metadata": {
        "id": "Ov1GobhGGHz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown In this exercise, we focus on the differences between excitatory and inhibitory transfer functions.\n",
        "\n",
        "# @markdown Now, we simulate the dynamics of one excitatory and one inhibitory population for a 10 seconds trial. Both populations are isolated (They are not connected to each other) and they recieve the same total input, as shown in the top panel. The toal input consists of a constant background input and an external stimulus that is presented between t=4 to t=6 seconds.\n",
        "\n",
        "# @markdown Change the background input and the stimulus intensity. Observe how the firing rate of each population reacts to the increased input.\n",
        "# @markdown 1. Slowly increase the background input from 0. Which population starts to fire first?\n",
        "\n",
        "# @markdown 2. Which population has a sharper increase in the firing rate? Which one is more active?\n",
        "\n",
        "# @markdown 3. What happens after the stimulus is removed?\n",
        "\n",
        "# @markdown Execute this cell to enable the widget!\n",
        "\n",
        "\n",
        "params = WorkingMemoryParameters()\n",
        "def inh_exploration( I_background = 0.3, I_stimulus = 0.2):\n",
        "  tau_R = 0.05\n",
        "  dt= 0.5e-3\n",
        "  T = 10\n",
        "  time_steps = int(T / dt)\n",
        "  stim_start = 4\n",
        "  stim_end = 6\n",
        "  I_ext = np.zeros(time_steps)\n",
        "  I_ext[int(stim_start/dt):int(stim_end/dt)] = I_stimulus\n",
        "\n",
        "\n",
        "  r_exc= simulate_excitatory_population(I_ext=I_ext,I_0= I_background, TAU_R= tau_R, dt=dt, T=T)\n",
        "  r_inh= simulate_inhibitory_population(I_ext=I_ext,I_0= I_background, TAU_R= tau_R, dt=dt, T=T)\n",
        "\n",
        "  I0 = np.ones(time_steps)*I_background\n",
        "  plt.figure(figsize=(11, 2))\n",
        "  plt.plot(np.linspace(0, T, time_steps), I0+ I_ext, label=\"Total Input\", color=\"green\")\n",
        "  plt.ylim((-0.6,1.6))\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Input current (nA)\")\n",
        "  plt.vlines(x=stim_start,ymin=-0.55, ymax=1.55,linestyles='dashed',color = \"black\")\n",
        "  plt.vlines(x=stim_end,ymin=-0.55, ymax=1.55,linestyles='dashed',color = \"black\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(11, 3))\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_exc, label=\"r (Excitatory)\")\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_inh, label=\"r (Inhibitory)\", color= \"grey\")\n",
        "  plt.vlines(x=stim_start,ymin=0.05, ymax=149,linestyles='dashed',color = \"black\")\n",
        "  plt.vlines(x=stim_end,ymin=0.05, ymax=149,linestyles='dashed',color = \"black\")\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Firing rate (Hz)\")\n",
        "  plt.legend()\n",
        "  plt.ylim((0,155))\n",
        "  plt.show()\n",
        "  #print(r_exc[int(2/dt)],r_exc[int(5/dt)])\n",
        "  #print(r_inh[int(2/dt)],r_inh[int(5/dt)])\n",
        "_ = widgets.interact(inh_exploration,I_background= (0.0,1,0.01), I_stimulus=(-0.6, 0.6, 0.1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D28r2YRJra0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Connecting populations with synaptic dynamics"
      ],
      "metadata": {
        "id": "OVlL_tH5ra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After learning to model individual populations of neurons, we are now ready to start connecting them together to form simple circuits.\n",
        "\n",
        "When populations are connected, we must consider how the presynaptic population’s firing rate influences the postsynaptic population through synaptic dynamics. Synaptic transmission is not instantaneous — it involves the release of neurotransmitters, their binding to receptors, and the opening of ion channels. This results in a change in conductance in the postsynaptic neuron, which evolves over time. (Although the conductance occurs on the postsynaptic side, in population models we associate the variable\n",
        "s with the presynaptic population, since it is directly driven by its firing rate r(t).)\n",
        "\n",
        "To capture these interactions, we introduce a new dynamic variable to represent synaptic conductance:\n",
        "\n",
        "* Excitatory populations activate NMDA receptors on the target population, producing an NMDA conductance variable $S^{N}$\n",
        "\n",
        "* Inhibitory populations activate GABA receptors, giving rise to a GABA conductance variable $S^{G}$.\n",
        "\n",
        "These conductances evolve over time according to their own differential equations, and they shape the synaptic current received by the postsynaptic population.\n",
        "\n",
        "If population A(excitatory) projects to population B, then:\n",
        "* $S_A^N$ represents the NMDA conductancee generated by A's firing.\n",
        "* The input current to B includes a term like: $$I_B \\sim J_{BA} . S_A$$\n",
        "Where $J_{BA}$ is the synaptic weight from A to B."
      ],
      "metadata": {
        "id": "g057eMgXra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NMDA dynamics\n",
        "\n",
        "The following equation models the dynamics of NMDA conducatance variable activated by excitatory cells:\n",
        "\n",
        "$$ \\frac{dS^N(t)}{dt} = -\\frac{S^N}{τ_N} + γ(1-S^N)r  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "NMDA receptor-mediated currents are slow and long-lasting. This is why they are important for temporal integration and working memory models.\n"
      ],
      "metadata": {
        "id": "gYVtgEgwra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GABA dynamics\n",
        "\n",
        "The following equation shows the dynamics of GABA conducatance variable:\n",
        "$$ \\frac{dS^G(t)}{dt} = -\\frac{S^G}{τ_G} + γ_Ir  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "GABA is fast-acting and responsible for rapid inhibition.\n"
      ],
      "metadata": {
        "id": "5lLsPpldra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.3"
      ],
      "metadata": {
        "id": "n4ta8RJbF_ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Execute this cell to enable the widget!\n",
        "\n",
        "# @markdown 1) Vary the time constants for NMDA and GABA within their valid range. Look at how each of the conductance variables change as a function of the firing rate of the population they belong to.\n",
        "\n",
        "# @markdown 2) What are the differences between NMDA and GABA conductance dynamics?\n",
        "params = WorkingMemoryParameters()\n",
        "def NMDA_conductance(params,I_ext=None,dt = 0.5e-3,T = 20,TAU_N= 0.06 ):\n",
        "       time_steps = int(T / dt)\n",
        "       # Initialize\n",
        "       S_A = np.zeros(time_steps)\n",
        "       r_A = np.zeros(time_steps)\n",
        "       S_A[0] = 0\n",
        "       for t in range(1, time_steps):\n",
        "              I_A =  params.I_0A + I_ext[t]\n",
        "              dr_A = (-r_A[t-1] + phi_E(I_A)) * (dt / params.TAU_R)\n",
        "              r_A[t] = r_A[t-1] + dr_A\n",
        "              S_A[t] = S_A[t-1] + dt * (-S_A[t-1] / TAU_N +\n",
        "                                          params.GAMMA * (1 - S_A[t-1]) * r_A[t])\n",
        "       return r_A, S_A\n",
        "def GABA_conductance(params,I_ext=None,dt = 0.5e-3,T = 20,TAU_G= 0.005 ):\n",
        "       time_steps = int(T / dt)\n",
        "       S = np.zeros(time_steps)\n",
        "       r = np.zeros(time_steps)\n",
        "       S[0] = 0\n",
        "       for t in range(1, time_steps):\n",
        "              I =  params.I_0A + I_ext[t]\n",
        "              dr = (-r[t-1] + phi_I(I)) * (dt / params.TAU_R)\n",
        "              r[t] = r[t-1] + dr\n",
        "              S[t] = S[t-1] + dt * (-S[t-1] / TAU_G +\n",
        "                                          params.GAMMA_I * r[t])\n",
        "       return r,S\n",
        "def F_exploration( TAU_N = 0.06, TAU_G = 0.005):\n",
        "\n",
        "  dt= 0.5e-3\n",
        "  T = 8\n",
        "  time_steps = int(T / dt)\n",
        "  I_ext_A = np.zeros(time_steps)\n",
        "  I_ext_A[int(1/dt):int(2/dt)] = 0.1\n",
        "  I_ext_A[int(2.5/dt):int(3/dt)] = 0.2\n",
        "  I_ext_A[int(4/dt):int(5/dt)] = 0.5\n",
        "  I_ext_A[int(6/dt):int(7/dt)] = 1\n",
        "\n",
        "  r,s= NMDA_conductance(params,I_ext=I_ext_A, dt=dt, T=T, TAU_N= TAU_N)\n",
        "  r2,s2= GABA_conductance(params,I_ext=I_ext_A, dt=dt, T=T, TAU_G= TAU_G)\n",
        "\n",
        "  plt.figure(figsize=(11, 1.5))\n",
        "  plt.plot(np.linspace(0, T, time_steps), I_ext_A, label=\"Input\",color = \"green\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(11, 1.5))\n",
        "  plt.plot(np.linspace(0, T, time_steps), r, label=\"r (Excitatory)\")\n",
        "  plt.plot(np.linspace(0, T, time_steps), r2, label=\"r (Inhibitory)\",color = \"grey\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.figure(figsize=(11, 1.5))\n",
        "  plt.plot(np.linspace(0, T, time_steps), s, label=\"s (NMDA)\")\n",
        "  plt.plot(np.linspace(0, T, time_steps), s2, label=\"s (GABA)\",color = \"grey\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  # plt.legend()\n",
        "  # plt.show()\n",
        "\n",
        "_ = widgets.interact(F_exploration, TAU_N=(0.05, 0.25, 0.01), TAU_G=(0.005, 0.020, 0.001))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ET7LfDLdra0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optional: AMPA dynamics\n",
        "Once you learn to model conductances, you can always extend your model to include more neurotransmitter dynamics. You can for example introduce $S^{AMPA}$ for excitatory populations to build a model that captures both the slow dynamics of NMDA as well as the fast dynamics of AMPA."
      ],
      "metadata": {
        "id": "2dyzxfPPra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bonus Exercise\n",
        "AMPA has similar dynamics to GABA but its effect on the postsynaptic neuron is excitatory. Write an equation describing the dynamics of $S^G$."
      ],
      "metadata": {
        "id": "2Jf4U0uRNrqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Local circuit model\n",
        "Now that we’ve built an understanding of how neural populations behave and how they can be connected via synaptic conductances, we can take the next step: constructing a simple circuit that captures key dynamics of a cortical brain area.\n",
        "\n",
        "We consider a minimal yet powerful architecture composed of two excitatory populations (A and B) and one inhibitory population (C). The excitatory populations are selective — each is tuned to a different input or stimulus feature — while the inhibitory population provides non-selective inhibition to both excitatory groups.\n",
        "\n",
        "This setup reflects a common organizational motif in the cortex, where pyramidal neurons form local subnetworks that are selectively responsive to particular inputs, and interneurons mediate competitive interactions between them. While real cortical networks contain many more such subpopulations, this reduced model retains the essential ingredients needed to study higher cognitive functions."
      ],
      "metadata": {
        "id": "ZeMHyZJrra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total synaptic input to any of the populations can be calculated using these equations:\n",
        "$$\n",
        "I_A = J_S S_A +J_C S_B + J_{EI} S_C + I_{ext_A} + I_{0A}\n",
        "$$\n",
        "$$\n",
        "I_B = J_C S_A +J_S S_B + J_{EI} S_C + I_{ext_B} + I_{0B}\n",
        "$$\n",
        "$$\n",
        "I_c = J_{IE} S_A +J_{IE} S_B + J_{II} S_C + I_{ext_C} + I_{0C}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "DDN-nrBwra0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where the synaptic conducatances are calculated as below:\n",
        "$$ \\frac{dS_A(t)}{dt} = -\\frac{S_A}{τ_N} + γ(1-S_A)r_A  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "$$ \\frac{dS_B(t)}{dt} = -\\frac{S_B}{τ_N} + γ(1-S_B)r_B  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "$$ \\frac{dS_C(t)}{dt} = -\\frac{S_C}{τ_G} + γ_Ir_c  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "And the firing rates of each population is calculated by solving:\n",
        "$$ \\tau_R \\frac{dr_A(t)}{dt} = -r_A(t) + Φ_E(I_A)  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "$$ \\tau_R \\frac{dr_B(t)}{dt} = -r_B(t) + Φ_E(I_B)  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
        "$$ \\tau_R \\frac{dr_C(t)}{dt} = -r_C(t) + Φ_I(I_C)  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n"
      ],
      "metadata": {
        "id": "wLUBQPRQTDTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation of the local circuit\n",
        "#@markdown Please run this cell before continueing to the exercises.\n",
        "params = WorkingMemoryParameters()\n",
        "def simulate_neural_mass_dynamics(params,I_ext_A=None,I_ext_B=None,I_ext_C=None,dt = 0.5e-3,T = 20):\n",
        "       time_steps = int(T / dt)\n",
        "       # Initialize\n",
        "       S_A = np.zeros(time_steps)\n",
        "       S_B = np.zeros(time_steps)\n",
        "       S_C = np.zeros(time_steps)\n",
        "       r_A = np.zeros(time_steps)\n",
        "       r_B = np.zeros(time_steps)\n",
        "       r_C = np.zeros(time_steps)\n",
        "       S_A[0] ,S_B[0] ,S_C[0] = 0 ,0,0\n",
        "       for t in range(1, time_steps):\n",
        "              # Inputs to each population\n",
        "              I_A = (params.J_S * S_A[t-1] + params.J_C * S_B[t-1] +\n",
        "                     params.J_EI * S_C[t-1] + params.I_0A + I_ext_A[t])\n",
        "              I_B = (params.J_C * S_A[t-1] + params.J_S * S_B[t-1] +\n",
        "                     params.J_EI * S_C[t-1] + params.I_0B) + I_ext_B[t]\n",
        "              I_C = (params.J_IE * S_A[t-1] + params.J_IE * S_B[t-1] +\n",
        "                     params.J_II * S_C[t-1] + params.I_0C) +I_ext_C[t]\n",
        "\n",
        "              # Compute the differentials of firing rates\n",
        "              dr_A = (-r_A[t-1] + phi_E(I_A)) * (dt / params.TAU_R)\n",
        "              dr_B = (-r_B[t-1] + phi_E(I_B)) * (dt / params.TAU_R)\n",
        "              dr_C = (-r_C[t-1] + phi_I(I_C)) * (dt / params.TAU_R)\n",
        "              # Update firing rates\n",
        "              r_A[t] = r_A[t-1] + dr_A\n",
        "              r_B[t] = r_B[t-1] + dr_B\n",
        "              r_C[t] = r_C[t-1] + dr_C\n",
        "              # Update state variables using Euler's method\n",
        "              S_A[t] = S_A[t-1] + dt * (-S_A[t-1] / params.TAU_N +\n",
        "                                          params.GAMMA * (1 - S_A[t-1]) * r_A[t])\n",
        "              S_B[t] = S_B[t-1] + dt * (-S_B[t-1] / params.TAU_N +\n",
        "                                          params.GAMMA * (1 - S_B[t-1]) * r_B[t])\n",
        "              S_C[t] = S_C[t-1] + dt * (-S_C[t-1] / params.TAU_G +\n",
        "                                          params.GAMMA_I * r_C[t])\n",
        "       return r_A, r_B, r_C"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sH1Bm6Oera0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.4\n",
        "\n",
        "In this exercise, we simulate a simple 10-second trial. Between t=4 and t=6, a stimulus is presented that matches the selectivity of population A. To model this, we apply an external input exclusively to population A during this time window.\n",
        "\n",
        "\n",
        "*   Use the slider to apply a brief external input to the excitatory population A.\n",
        "*   Observe how **both populations** respond **during** and **after** the stimulus.\n",
        "* Gradually increase $J_S$. At what point do you notice a qualitative change in the network? at what exact value of $J_S$ do you notice this change?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4FG3DatA4D5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: When adjusting $J_s$, the inhibitory coupling $J_{IE}$ is updated accordingly to preserve the balance between excitation and inhibition in the network."
      ],
      "metadata": {
        "id": "u4h0SYLMpa6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Execute this cell to enable the widgets!\n",
        "\n",
        "\n",
        "\n",
        "def F_exploration(stimulus_A = 0.2,J_s = 0.32):\n",
        "  params = WorkingMemoryParameters()\n",
        "  dt= 0.5e-3\n",
        "  T = 10\n",
        "  time_steps = int(T / dt)\n",
        "  stim_start = 4\n",
        "  stim_end = 6\n",
        "  I_ext_A = np.zeros(time_steps)\n",
        "  I_ext_A[int(stim_start/dt):int(stim_end/dt)] = stimulus_A\n",
        "  params.update(J_S=J_s)\n",
        "  def calc_J_IE():\n",
        "        return (params.J_0 - params.J_S - params.J_C) / (2 * params.J_EI * params.ZETA)\n",
        "  j_ie = calc_J_IE()\n",
        "  params.update(J_IE=j_ie)\n",
        "  r_A, r_B, r_C = simulate_neural_mass_dynamics(params,I_ext_A=I_ext_A,I_ext_B=np.zeros(time_steps),I_ext_C=np.zeros(time_steps), dt=dt, T=T)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(15, 8))\n",
        "\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_A,linewidth = 2.0 ,label=\"r_A (Excitatory A)\")\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_B,linewidth = 2.0 ,label=\"r_B (Excitatory B)\",color = \"purple\")\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_C,linewidth = 1.2 ,label=\"r_C (Inhibitory C)\",color = \"grey\")\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "_ = widgets.interact(F_exploration, stimulus_A=(0.0, 0.2, 0.001), J_s = (0.1, 1, 0.01))"
      ],
      "metadata": {
        "id": "9A5lrH_wra0p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attractory dynamics\n",
        "As you may have noticed, this simple circuit creates competition between the two selective excitatory populations through shared inhibition. When one population becomes active, it suppresses the other via the inhibitory population. This mutual inhibition can give rise to winner-take-all dynamics, where only one population remains active at a time.\n",
        "\n",
        "Such dynamics are a hallmark of attractor networks—systems that tend to settle into one of multiple stable activity patterns (or attractors), depending on initial conditions or inputs. These stable states can represent decisions, percepts, or memories."
      ],
      "metadata": {
        "id": "ZUjK2z3era0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.5\n",
        "In the simulation below, a stimulus is presented at t = 3s that selectively excites population A. Later, at t = 7s, a distractor is presented to population B. You can adjust the strength of both the stimulus and the distractor to see how the network responds.\n",
        "\n",
        "As you explore, consider:\n",
        "\n",
        "* What happens if the initial stimulus is strong, but the distractor is weak?\n",
        "\n",
        "* At what point does the distractor become strong enough to disrupt the ongoing activity?\n",
        "\n",
        "* How does the network behave after the inputs are removed—does it return to baseline, or remain in an active state?\n",
        "\n",
        "* Does the firing rate in the active state depend on the intensity of the stimulus or the distractor?\n",
        "\n",
        "These questions will help you uncover how attractor dynamics enable the network to exhibit decision-making and working memory-like behavior."
      ],
      "metadata": {
        "id": "Zk-XGMH8qCl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Execute this cell to enable the widget!\n",
        "params = WorkingMemoryParameters()\n",
        "def F_exploration(I_stimulus = 0.2,I_distractor = 0.1):\n",
        "  params.update(J_S=0.47)\n",
        "  def calc_J_IE():\n",
        "        return (params.J_0 - params.J_S - params.J_C) / (2 * params.J_EI * params.ZETA)\n",
        "  j_ie = calc_J_IE()\n",
        "  params.update(J_IE=j_ie)\n",
        "  dt= 0.5e-3\n",
        "  T = 10\n",
        "  time_steps = int(T / dt)\n",
        "  stim_start = 3\n",
        "  stim_end = 4\n",
        "  _I_ext_A = np.zeros(time_steps)\n",
        "  _I_ext_A[int(stim_start/dt):int(stim_end/dt)] = I_stimulus\n",
        "  _I_ext_B = np.zeros(time_steps)\n",
        "  _I_ext_B[int(7/dt):int(8/dt)] = I_distractor\n",
        "  _I_ext_C = np.zeros(time_steps)\n",
        "\n",
        "\n",
        "  r_A, r_B, r_C = simulate_neural_mass_dynamics(params,I_ext_A=_I_ext_A, I_ext_B= _I_ext_B, I_ext_C= _I_ext_C, dt=dt, T=T)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(15, 8))\n",
        "\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_A, label=\"r_A (Excitatory A)\")\n",
        "  plt.plot(np.linspace(0, T, time_steps), r_B, label=\"r_B (Excitatory B)\",color= \"purple\")\n",
        "  #plt.plot(np.linspace(0, T, time_steps), r_C, label=\"r_C (Inhibitory C)\",color = \"grey\")\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "_ = widgets.interact(F_exploration, I_stimulus=(0.0, 1, 0.01), I_distractor=(0.0, 0.2, 0.005))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GHKuo4w-ra0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Structural Connectivity"
      ],
      "metadata": {
        "id": "Yid31v0OXVGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background: the Connectome"
      ],
      "metadata": {
        "id": "Yqd9jBEq5Lio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the connections made by cortical neurons are with their nearest neighbors (within millimeters of the soma). However, pyramidal neurons can project to distant areas of the cortex through myelinated axons, which bundle together in white-matter fibers (https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.12271). The network of white-matter tracts is what network neuroscientists call **the connectome**. The connectome can be used to connect neural-masses, such as the ones developed in the first part of the tutorial, and is, therefore, one of the key ingredients in building **large-scale brain models**.\n",
        "\n",
        "In humans, white-matter tracts are normally measured noninvasively with diffusion-weighted magnetic resonance imaging, which estimates white-matter tracts from the direction of water diffusion. However, this estimation is susceptible to measurement noise, under-estimates white-matter tracts at a greater depth within the brain and does not provide information about the directionality of axons within white-matter tracts.\n",
        "\n",
        "Conversely, in non-human primates, other invasive methods can be used, allowing for a more precise estimation of long-range cortical connections. One of such methods is **retrograde tracing**, whereby a tracer is injected in a cortical area and travels from the synapses in the area towards the soma of the presynaptic neurons. Therefore, by injecting in a given area and observing where the tracer ends up, this technique indicates which neurons (sources) project to the injected area (target). For more information, please consult https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1111/nyas.12271.\n",
        "\n",
        "In this tutorial, to define cortical areas, we use the M132 brain parcellation, detailed in https://doi.org/10.1093/cercor/bhs270, with 91 areas in the left hemisphere defined by histological characteristics. A plot of this parcellation can be visualized below (adapted from https://academic.oup.com/cercor/article/24/1/17/272931).\n",
        "\n",
        "&nbsp;\n",
        "![Representation of the M132 parcellation of the macaque brain with specific colors for each area](https://drive.google.com/uc?id=10_73Ru-CYnrFGTHiIAQZktW8auG0QySl \"M123 Parcellation of the Macaque Brain\")"
      ],
      "metadata": {
        "id": "QNky5QXjXaPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good way to approximate the strength of connectivity from area $j$ to area $i$ with tract tracing data is to count the number of labeled neurons in area $j$ after injection in area $i$.\n",
        "\n",
        "This quantity can then be divided by the total number of neurons projecting to area $i$ to obtain the Fraction of Labeled Neurons (FLN), which indicates the strength of the projections from area $j$ to $i$. Note that, since this metric is based on tract tracing data, which has a clear source and target, the resulting FLN matrix is directed (i.e. the connection from $i$ to $j$ is not necessarily the same as $j$ to $i$)."
      ],
      "metadata": {
        "id": "9dPE0ZXcYWi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run the following cell to load FLN data from injections in 30 areas of the macaque connectome, together with a description of the 30 areas.\n",
        "\n",
        "# @markdown **Pick one of the areas now and pay attention to its features when completing the next exercises!**\n",
        "\n",
        "\n",
        "FLN, SLN, area_centers, area_info, h = load_structural_data()\n",
        "\n",
        "area_labels = area_info[\"Name\"].values\n",
        "\n",
        "distances = np.zeros((area_centers.shape[0], area_centers.shape[0]))\n",
        "for i in range(distances.shape[0]):\n",
        "    for j in range(i+1, distances.shape[1]):\n",
        "        distances[i, j] = np.linalg.norm(area_centers[j, :] - area_centers[i, :])\n",
        "\n",
        "distances += distances.T\n",
        "\n",
        "area_info"
      ],
      "metadata": {
        "id": "KVcg5A7aXIK9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's take a look at the FLN matrix. Run this cell to plot it in matrix form.\n",
        "\n",
        "plt.figure(figsize = (15, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(FLN, cmap = 'inferno')\n",
        "c = plt.colorbar()\n",
        "c.set_label('FLN')\n",
        "plt.xticks(np.arange(FLN.shape[0]), area_labels, rotation = 45, ha = 'right', fontsize = 7)\n",
        "plt.yticks(np.arange(FLN.shape[0]), area_labels, fontsize = 7)\n",
        "plt.xlabel('From')\n",
        "plt.ylabel('To')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(np.log10(FLN), cmap = 'inferno')\n",
        "c = plt.colorbar()\n",
        "c.set_label('log10(FLN)')\n",
        "plt.xticks(np.arange(FLN.shape[0]), area_labels, rotation = 45, ha = 'right', fontsize = 7)\n",
        "plt.yticks(np.arange(FLN.shape[0]), area_labels, fontsize = 7)\n",
        "plt.xlabel('From')\n",
        "plt.ylabel('To')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KhVdatYQZcP8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.1. Visualizing the connectome"
      ],
      "metadata": {
        "id": "dram1u2y2CeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's now plot the node's positions in the brain together with their connectivity. The thickness of the lines represent the weight of the connection.\n",
        "\n",
        "# @markdown With the slider, you can regulate what percentage of strongest connections is shown. For example, if the threshold is 10, only the 10% strongest connections are shown. Try to move it around and see how it changes the visualization (it takes some time to update).\n",
        "\n",
        "# @markdown 1. Are there any interesting features that you notice already from this plot and the matrix representation? Are there major changes in the overall structure of the connectome when applying a more stringent threshold?\n",
        "def threshold_connectivity(Threshold = 100):\n",
        "\n",
        "  plot_matrix(matrix = FLN, coordinates = area_centers, node_labels = area_labels, threshold=Threshold/100)\n",
        "\n",
        "_ = widgets.interact(threshold_connectivity, Threshold=(2, 100, 2))"
      ],
      "metadata": {
        "id": "nINBsQtMZymP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the matrix is relatively dense (about 65% of possible connectione exist), strong connectivity is quite sparse, with a large majority of connections having a very low weight."
      ],
      "metadata": {
        "id": "xtgk_ApOc-Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's plot the distribution of non-zero FLN values to better visualize this \"sparsity\".\n",
        "\n",
        "connections = FLN.flatten()\n",
        "connections = connections[connections != 0]\n",
        "\n",
        "plt.figure(figsize = (8, 3), dpi = 150)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "y, x = np.histogram(connections, bins = np.arange(0, 1.01, 0.02))\n",
        "x = 0.5 * (x[1:] + x[:-1])\n",
        "plt.plot(x, y, color = 'k', linewidth = 1)\n",
        "plt.ylabel('# Connections')\n",
        "plt.xlabel('FLN')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "y, x = np.histogram(connections, bins = np.arange(0, 1.01, 0.02))\n",
        "x = 0.5 * (x[1:] + x[:-1])\n",
        "plt.plot(x, y, color = 'k', linewidth = 1)\n",
        "plt.ylabel('# Connections')\n",
        "plt.xlabel('FLN')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sK9TBZ3ucXzV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might notice that, when plotting the distribution of non-zero connection weights in a log-log plot, it seems to follow a linear relationship. This type of scaling is called a **power-law** and it can be found ubiquitously across different brains (https://doi.org/10.1103/PhysRevResearch.7.013134) and other complex systems (https://www.science.org/doi/full/10.1126/science.284.5420.1677)."
      ],
      "metadata": {
        "id": "hq1sA7I7dF4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.2. Understanding node degrees\n"
      ],
      "metadata": {
        "id": "lW5dMS3_dZU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start our exploration of the connectome by calculating one of the simples features of nodes in a network: their **degree**. In unweighted networks (where connections are binary - either present or absent), the degree of a node is the number of connections, or links, it establishes with the network. This concept can be adapted to weighted networks by simply computing the sum of connection weights linked to a node.\n",
        "\n",
        "Furthermore, since we are dealing with a directed network (W), there are two ways of computing weighted node degrees:\n",
        "\n",
        "- **in-degree:** number of incoming connections to a node\n",
        "\n",
        "$$\n",
        "\\text{in-degree}_i = \\sum_j W_{ij}\n",
        "$$\n",
        "\n",
        "- **out-degree:** number of outgoing connections from a node\n",
        "\n",
        "$$\n",
        "\\text{out-degree}_j = \\sum_i W_{ij}\n",
        "$$"
      ],
      "metadata": {
        "id": "7zOXFNjvd-Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's now compare weighted in- and out- degrees (i.e. sum of incoming and out-going connections) in our network. Execute this cell to get an interactive plot of in- and out- degrees.\n",
        "\n",
        "\n",
        "# @markdown 1. Are nodes generally balanced in terms ou in- and out- degrees? Do you think this can influence the role of cortical areas in the network?\n",
        "\n",
        "def compute_degree(W, degree_type = 'In'):\n",
        "    \"\"\"\n",
        "    Computes the in-degree (sum of incoming connections) for all nodes in a network\n",
        "\n",
        "    Args:\n",
        "      W (array): connectivity matrix\n",
        "      degree_type (str): type of degree to be computed. Options: ['In', 'Out']\n",
        "\n",
        "    Returns:\n",
        "      in_degree (array): node degree\n",
        "    \"\"\"\n",
        "    ###################################################################################################################################################\n",
        "    # Exercise 1: Fill out the first in_degree = np.sum(...) and comment the line below before running this cell\n",
        "    # Hint: the np.sum function has an argument axis that allows for the values of a matrix to be summed along a given axis.\n",
        "    ###################################################################################################################################################\n",
        "    # Exercise 2: Fill out the second in_degree = np.sum(...) and comment the line below before running this cell\n",
        "    # Hint: try to think of a simple operation that tells you if there is a connection or not. In python, booleans (False/True) will be interpreted as integers (0/1) if used in mathetmatical operations\n",
        "    ###################################################################################################################################################\n",
        "    # raise NotImplementedError(\"Please complete coding exercise\")\n",
        "    ###################################################################################################################################################\n",
        "\n",
        "    if degree_type == 'In':\n",
        "      degree = np.sum(W, axis = 1)\n",
        "      #degree = np.sum(W!=0, axis = 1).astype(float)\n",
        "    elif degree_type == 'Out':\n",
        "      degree = np.sum(W, axis = 0)\n",
        "      #degree = np.sum(W!=0, axis = 0).astype(float)\n",
        "\n",
        "    return degree\n",
        "\n",
        "def plot_degree(Degree_Type = ['In', 'Out']):\n",
        "\n",
        "  degree = compute_degree(FLN, degree_type = Degree_Type)\n",
        "\n",
        "  plot_matrix(matrix = FLN, coordinates = area_centers, node_sizes = 4*degree/np.nanmax(degree), node_labels = area_labels, threshold=0.1)\n",
        "\n",
        "_ = widgets.interact(plot_degree)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CnZ1sWGDig5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, by comparing the plots with the in- and out-degrees, the strength of incoming and outgoing connections is not always balanced. Therefore, by measuring the difference between in- and out- degrees, it is possible to visualize if nodes are mostly information \"senders\" (out-degree > in-degree) or \"receivers\" (out-degree < in-degree). More information on the functional role of senders and receivers and how to infer them in the undirected human connectome, refer to https://www.nature.com/articles/s41467-019-12201-w."
      ],
      "metadata": {
        "id": "vew9hWBhj7Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to plot the difference between in- and out- degrees across our network. In the brain network plot, node colors represent the normalized difference between in- and out- degrees.\n",
        "\n",
        "# @markdown 1. Is your chosen area a sender or a receiver? Do you think senders and receivers play a different role in brain function?\n",
        "\n",
        "\n",
        "in_degree = compute_degree(FLN, degree_type = 'In')\n",
        "out_degree = compute_degree(FLN, degree_type = 'Out')\n",
        "\n",
        "in_out_ratio = (in_degree - out_degree)/(in_degree + out_degree)\n",
        "\n",
        "plt.figure(figsize = (15.04, 3), dpi = 100)\n",
        "plt.plot(np.sort(in_out_ratio), 'ok')\n",
        "plt.xticks(np.arange(len(in_out_ratio)), [area_labels[n] for n in np.argsort(in_out_ratio)], rotation = 45, ha = 'right')\n",
        "plt.axhline(y = 0, color = 'k', alpha = 0.2)\n",
        "\n",
        "i = np.where(np.sort(in_out_ratio) > 0)[0][0]-0.5\n",
        "plt.fill_between([-10, i], [-10, -10], [10, 10], color = 'teal', alpha = 0.2)\n",
        "plt.fill_between([i, 1000], [-10, -10], [10, 10], color = 'darkred', alpha = 0.2)\n",
        "plt.text(0.5 * i, 0.5, 'Senders', ha = 'center', fontsize = 15)\n",
        "plt.text(0.5 * (i + len(in_out_ratio)), -0.5, 'Receivers', ha = 'center', fontsize = 15)\n",
        "\n",
        "plt.xlim([-0.5, len(in_out_ratio)-0.5])\n",
        "plt.ylim([-1, 1])\n",
        "plt.ylabel(f'In Degree - Out Degree\\n(Normalized)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plot_matrix(matrix = FLN, coordinates = area_centers, node_colors = in_out_ratio, node_cmap = 'RdBu_r', node_label_color = 'k', node_labels = area_labels, threshold=0.1)"
      ],
      "metadata": {
        "id": "2pO7az9MkIai",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Bonus Exercise - Difference between weighted and unweighted degree**"
      ],
      "metadata": {
        "id": "OVJdH0AWvBKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run the following cell to visualize the in-degree of nodes. In this case, the size of nodes will represent their in-degree. The widget allows you to choose between the weighted and unweighted version.\n",
        "\n",
        "# @markdown 1. Visualize the weighted and unweighted in-degrees of our areas. Do you notice any differences?\n",
        "\n",
        "def compute_in_degree(W, weighted = True):\n",
        "    \"\"\"\n",
        "    Computes the in-degree (sum of incoming connections) for all nodes in a network\n",
        "\n",
        "    Args:\n",
        "      W (array): connectivity matrix\n",
        "      weighted (bool): if True, accounts for the weight of links when computing in-degrees. Otherwise, it just counts the number of incoming connections\n",
        "\n",
        "    Returns:\n",
        "      in_degree (array): in-degree\n",
        "    \"\"\"\n",
        "    ###################################################################################################################################################\n",
        "    # Exercise 1: Fill out the first in_degree = np.sum(...) and comment the line below before running this cell\n",
        "    # Hint: the np.sum function has an argument axis that allows for the values of a matrix to be summed along a given axis.\n",
        "    ###################################################################################################################################################\n",
        "    # Exercise 2: Fill out the second in_degree = np.sum(...) and comment the line below before running this cell\n",
        "    # Hint: try to think of a simple operation that tells you if there is a connection or not. In python, booleans (False/True) will be interpreted as integers (0/1) if used in mathetmatical operations\n",
        "    ###################################################################################################################################################\n",
        "    # raise NotImplementedError(\"Please complete coding exercise\")\n",
        "    ###################################################################################################################################################\n",
        "\n",
        "    if weighted:\n",
        "      in_degree = np.sum(W, axis = 1)\n",
        "    else:\n",
        "      in_degree = np.sum(FLN != 0, axis = 1).astype(float)\n",
        "\n",
        "    return in_degree\n",
        "\n",
        "\n",
        "def plot_degree(Weighted = False):\n",
        "\n",
        "  in_degree = compute_in_degree(FLN, weighted = Weighted)\n",
        "\n",
        "  plot_matrix(matrix = FLN, coordinates = area_centers, node_sizes = 4*in_degree/np.nanmax(in_degree), node_labels = area_labels, threshold=0.1)\n",
        "\n",
        "_ = widgets.interact(plot_degree)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KdytYmiTf9W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background: Modular Organization in the Connectome"
      ],
      "metadata": {
        "id": "FzYgesMdl7Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now looked at simple features of network nodes which clarify a bit their role within the network through the strength of their connections. However, such complex networks have characteristics that are beyond how strong their connections are.\n",
        "\n",
        "One of such characteristics is **modularity** - the tendency of a network to organize in sub-networks of nodes that are strongly connected with each other and more weakly connected to the rest of the network. There is extensive evidence that structural (and functional) brain networks follow to a modular architecture, which is thought to minimize the cost of wiring and allow for module specialization, which is not only advantageous for behavior, but also to confine the spread of damage in the network (https://www.nature.com/articles/s41583-019-0177-6).\n",
        "\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "\n",
        "There are several ways to detect modules in networks (e.g. K-means clustering of connectivity). Here, we use a built-in function of the *networkx* package in python made for detecting community structure in networks that can be weighted and directed, such as ours: *networkx.community.louvain_communities*. This method relies in the following way of computing the degree of **modularity** ($Q$) of a network:\n",
        "\n",
        "$$\n",
        "Q = \\frac{1}{m} \\sum_{i,j}\\left[ W_{ij} - \\frac{\\text{d}_i\\text{d}_j}{m} \\right] \\delta(\\text{module}_i, \\text{module}_j)\n",
        "$$\n",
        "\n",
        "\n",
        "where $W_{ij}$ is the connectivity matrix, $d_i$ is the degree of node $i$, $m$ is $\\sum_{i,j}W_{i,j}$ and $\\delta(\\text{module}_i, \\text{module}_j)$ is a Dirac delta function that is 1 if $i$ and $j$ belong to the same module and 0 otherwise. For more details, consult https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008.\n",
        "\n",
        "In short terms, this method measures **how strongly nodes within a module are connected to each other in comparison to their connectivity to the rest of the network**. While this specific formula works for undirected networks, it can be adapted to the use-case of directed networks such as ours (see *networkx* documentation).\n",
        "\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "\n",
        "To detect network communities, *networkx.community.louvain_communities* starts with all nodes belonging to their own community. Then, each node is moved to a different community and the move that results in a greatest increase in $Q$ is then kept as a new module. After this is repeated for all nodes, the algorithm is re-run in a network where each node represents a community obtained from the previous step. This process is repeated iteratively until no substantial gain in modularity can be obtained by reorganizing modules, reaching a maximum level of modularity"
      ],
      "metadata": {
        "id": "U0UGj_1mmOv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's now look at the communities detected in our FLN network by this method. Execute this cell to run the Louvain community detection algorithm in our network and visualize the results.\n",
        "\n",
        "# @markdown 1. You might notice that areas belonging to the same module (which is determined solely by their connectivity patterns) tend to also be clustered spatially in the brain, can you think about why it might be the case?\n",
        "\n",
        "\n",
        "G = nx.from_numpy_array(FLN.T, create_using=nx.DiGraph)\n",
        "mod_res = nx.community.louvain_communities(G, seed = 42)\n",
        "\n",
        "modules = np.zeros(len(area_labels))\n",
        "for i, m in enumerate(mod_res):\n",
        "    for n in m:\n",
        "        modules[n] = i\n",
        "\n",
        "N_modules = int(np.max(modules))+1\n",
        "\n",
        "for i in range(N_modules):\n",
        "    print(f'Module {i}')\n",
        "    print([area_labels[n] for n in range(len(area_labels)) if modules[n] == i])\n",
        "    print('')\n",
        "\n",
        "\n",
        "\n",
        "plot_matrix(FLN, area_centers, node_colors = modules, node_cmap = 'jet', node_labels=area_labels, node_label_color='k')"
      ],
      "metadata": {
        "id": "d867dzFAh_CL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's explore a possible explanation for the spatial clustering of nodes that area strongly connected. Run this cell to generate a plot of the FLN weight between two areas vs the Euclidean distance between their centers.\n",
        "\n",
        "plt.figure(figsize = (10, 3), dpi = 150)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "\n",
        "plt.scatter(distances[FLN != 0], FLN[FLN != 0], alpha = 0.2)\n",
        "r, p = stat.pearsonr(distances[FLN != 0], FLN[FLN != 0])\n",
        "plt.text(120, 0.5, f\"Pearson's r = {r:.3f}\\np =  {p:.3f}\", ha = 'right')\n",
        "plt.xlabel('Euclidean Distance (mm)')\n",
        "plt.ylabel('FLN')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(distances[FLN != 0], FLN[FLN != 0], alpha = 0.2)\n",
        "r, p = stat.pearsonr(distances[FLN != 0], np.log10(FLN[FLN != 0]))\n",
        "plt.text(120, 0.5, f\"Pearson's r = {r:.3f}\\np =  {p:.3f}\", ha = 'right', va = 'top')\n",
        "plt.xlabel('Euclidean Distance (mm)')\n",
        "plt.ylabel('FLN')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uKSAYTrCmo5W",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While their relationship might not be obvious when looking at a normal plot, the picture is clearer when plotting the log of FLN instead. This suggests that there is an exponential relationship between the distance between two areas in the brain and how strongly they connect. This exponential relation is a well known feature of brain organization, particularly at larger-scales (https://www.cell.com/neuron/fulltext/S0896-6273(13)00660-0). Since areas that area close together in space tend to connect more strongly, the modules defined by connectivity will also have this tendency to include areas that are close together in the brain.\n",
        "\n",
        "Interestingly, there are some unusually strong long-range connections beyond this exponential decay rule that are thought to be quite influential in large-scale brain dynamics (https://doi.org/10.1073/pnas.2415102122)."
      ],
      "metadata": {
        "id": "Rcw9V0Prjw5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.3. Visualizing and understanding participation coefficients\n",
        "\n"
      ],
      "metadata": {
        "id": "5H0o-bHHoXrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have divided our network into modules, let's explore an interesting metric to quantify the \"importance\" of a node within the network: the **participation coefficient**, which quantifies how \"uniformly-distributed\" the links of a node are across different modules. A node with a high participation coefficient can be seen as a brain area that streamlines communication between different modules, while another with low participation coefficient will mostly interact with areas belonging to the same module.\n",
        "\n",
        "Here's how the participation coefficient of node $i$ ($P_i$) can be calculated:\n",
        "\n",
        "$$\n",
        "P_i = 1 - \\sum_{m} \\left( \\frac{\\text{degree}_{i,m}}{\\text{degree}_i} \\right)^2\n",
        "$$\n",
        "\n",
        "where $\\text{degree}_{i,m}$ is the sum of connections between node $i$ and nodes belonging to module $m$."
      ],
      "metadata": {
        "id": "iXGpaxKMoenD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to visualize the participation coefficient of nodes. For simplicity, the results you will see relate to an undirected version of FLN, where link $ij$ is the average of links $ij$ and $ji$ in the original FLN Matrix.\n",
        "\n",
        "# @markdown In the plot, colors represent the different modules and the size of nodes represents their participation coefficient.\n",
        "\n",
        "# @markdown 1. Do you notice anything different when visualizing participation coefficients vs node degrees? Which one of the metrics do you think is more useful to study how different nodes shape the behavior of a network?\n",
        "\n",
        "\n",
        "def compute_participation_coefficient(W, modules):\n",
        "    \"\"\"\n",
        "    Computes the participation coefficient of a network with defined modules\n",
        "\n",
        "    Args:\n",
        "      W (array): connectivity matrix\n",
        "      modules (array): array with the module associated with each node\n",
        "\n",
        "    Returns:\n",
        "      pc (array): participation coefficient\n",
        "    \"\"\"\n",
        "    ###################################################################################################################################################\n",
        "    # Exercise 1: Fill out the first degree_mod = ... and comment the line below before running this cell\n",
        "    # Hint: you can use the variable modules as mask to known which module a node belongs to. For example, modules == 2 returns an array that is True for all nodes belonging to module 2.\n",
        "    ###################################################################################################################################################\n",
        "    # raise NotImplementedError(\"Please complete coding exercise\")\n",
        "    ###################################################################################################################################################\n",
        "\n",
        "    degree = compute_degree(W) # in the following example, it does not matter if we use in or out-degrees because the matrix wil be symmetric\n",
        "\n",
        "    pc = np.zeros(W.shape[0])\n",
        "    for i in range(len(pc)):\n",
        "        sum = 0\n",
        "        for m in range(N_modules):\n",
        "            degree_mod = np.sum(W[i, modules == m])\n",
        "            sum += (degree_mod/degree[i])**2\n",
        "        pc[i] = 1 - sum\n",
        "\n",
        "    return pc\n",
        "\n",
        "\n",
        "FLN_symmetric = 0.5 * (FLN + FLN.T) # average of in and out connections, for simplicity (avoids having to account separately for in and out degrees)\n",
        "\n",
        "def plot_deg_or_pc(Node_Size = ['Degree', 'Participation Coefficient']):\n",
        "\n",
        "  pc = compute_participation_coefficient(FLN_symmetric, modules)\n",
        "  dg = compute_degree(FLN_symmetric)\n",
        "\n",
        "  if Node_Size == 'Degree':\n",
        "      plot_matrix(FLN, area_centers, node_colors = modules, node_sizes = 4*dg/np.nanmax(dg), node_cmap = 'jet', node_labels=area_labels, node_label_color='k')\n",
        "  elif Node_Size == 'Participation Coefficient':\n",
        "      plot_matrix(FLN, area_centers, node_colors = modules, node_sizes = 4*pc/np.nanmax(pc), node_cmap = 'jet', node_labels=area_labels, node_label_color='k')\n",
        "\n",
        "\n",
        "_ = widgets.interact(plot_deg_or_pc)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YIozqtL5p1aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background: Hierarchical Organization of Long-Range Connectivity"
      ],
      "metadata": {
        "id": "dECB4PiT-XdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is one feature of the cortex that we haven't accounted for, so far: it's **laminar architecture**.\n",
        "\n",
        "The mammalian cortex is organized in layers (typically 6), with specific patterns of interconnectivity that define the basis of the canonical cortical microcircuit (see https://ieeexplore.ieee.org/abstract/document/6796535 for a seminal paper on the canonical microcircuit of the cortex). A cytoarchitectonic representation of the layered structure of the human cortex can be seen in the image below (from https://neurology.mhmedical.com/book.aspx?bookID=3024). Layers can also be grouped by their relative position to layer IV:\n",
        "\n",
        "- **Superficial or supragranular**: layers I, II and III\n",
        "- **Granular**: layer IV\n",
        "- **Deep or infragranular**: layers V and VI\n",
        "\n",
        "&nbsp;\n",
        "![Cytoarchitectonic view of the laminar structure of the cortex across different areas](https://drive.google.com/uc?id=13iFzIK3Xd7mDp3QAV6ZicEgyuTisSyaw \"Layers of the cortical microcircuit in different areas\")\n",
        "\n",
        "\n",
        "Why is this laminar structure relevant when talking about the connectome? When looking at the organization of white-matter tracts, while accounting for the laminar structure of the cortex, a **\"hierarchical\"** pattern becomes apparent:\n",
        "\n",
        "Projections from sensory to association areas, or **feedforward projections** tend to originate from **superficial layers** and target excitatory neurons in layer IV, which relay the signal mostly to local superficial layers. Conversely, in the opposite direction, or **feedback projections** originate in **deep layers** and have more diffuse targets, reaching excitatory and inhibitory neurons across superficial and deep layers (see https://doi.org/10.1093/cercor/1.1.1-a and https://doi.org/10.1126/sciadv.1601335 for more details).\n",
        "\n",
        "&nbsp;\n",
        "![Simplified view of laminar connectivity patterns in the cortex](https://drive.google.com/uc?id=1FQzp5v4lOX5ZQeUC4PqV2FdKI8_brcDA \" Simplified view of laminar connectivity patterns in the cortex\")\n",
        "\n",
        "\n",
        "Fortunately, these patterns can be measured with **retrograde tracing**! If a tracer is injected in V1, for example, we can count the number of number of neurons labeled in the superficial layers of area V4 and divide it by the total number of neurons found in area V4. This quantity is called the fraction of **supragranular labeled neurons**, or **SLN** and can inform about the relative position of these two areas in the feedforward and feedback flows of information."
      ],
      "metadata": {
        "id": "lHCtN_7l-Xar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained previously, connections from sensory to higher-order areas originate mostly from superficial layers (high SLN), while the opposite is the case in the opposite direction. This asymmetry can be used to determine a \"hierarchical\" organization of cortical areas. In short, the method used in https://www.cell.com/neuron/fulltext/S0896-6273(15)00765-5 optimizes the hierarhical value of each area that allows for the best prediction of the empirically obtained SLN values. With the resulting hierarchy values, one can have a better idea of where different areas of the cortex stand in the flow of information from early sensory to higher-order association areas.\n",
        "\n"
      ],
      "metadata": {
        "id": "JzMF2XUmBAya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to visualize the hierarchy across the brain.\n",
        "\n",
        "plot_matrix(FLN, area_centers, node_colors = h, node_cmap = 'inferno', node_labels=area_labels, node_label_color='k')"
      ],
      "metadata": {
        "id": "MGPNujr_BGPo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.4: Understanding Counterstream Inhibition"
      ],
      "metadata": {
        "id": "N8bGjZAJ5fuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model, however, does not have the laminar structure that could support the hierarchical laminar organization of long-range connectivity measured through SLN.\n",
        "\n",
        "However, there is a way of approaching some of the functional consequences of this organization.\n",
        "\n",
        "A purely feedfoward connection is considered to have an excitatory effect, since it stems from excitatory neurons in superficial layers and targets excitatory neurons. Conversely, feedback connections have a more diffusive pattern of connectivity and target inhibitory as well as excitatory neurons. Therefore, our model is built on the hypothesis that feedback connections have a net inhibitory effect on the target population and thus, would target mostly interneurons (consult https://elifesciences.org/articles/72136 for more details on how this organization contributes to the stabilization of global dynamics and the propagation of information across the hierarchy).\n",
        "\n",
        "Because connections are generally not purely feedforward or feedback, we use the following formulas to compute the strenght of long range projections reaching either excitatory ($W_{ij}^E$) or inhibitory (($W_{ij}^I$)) neurons in the target population:\n",
        "\n",
        "&nbsp;\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "$$\n",
        "W_{ij}^E = SLN_{ij} * FLN_{ij}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{ij}^I = (1 - SLN_{ij}) * FLN_{ij}\n",
        "$$"
      ],
      "metadata": {
        "id": "5j-mv_4q-XYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to visualize the excitatory targetting and inhibitory targetting components of long-range connectivity.\n",
        "\n",
        "# @markdown 1. Try to visualize first E->E and then E->I projections. Do they seem generally balanced?\n",
        "\n",
        "# @markdown 2. Choose the option \"Excitatory - Inhibitory\", which will show you the difference between E->E and E->I connections. Do you notice some directionality in certain groups of areas?\n",
        "\n",
        "\n",
        "def plot_counterstream(Connection_Type = 'Excitatory'):\n",
        "\n",
        "  if Connection_Type == 'Excitatory':\n",
        "      matrix = FLN * SLN\n",
        "      plot_matrix_EI(matrix = matrix, coordinates = area_centers, node_labels = area_labels, threshold=0.1)\n",
        "\n",
        "  if Connection_Type == 'Inhibitory':\n",
        "      matrix = FLN * (1 - SLN)\n",
        "      plot_matrix_EI(matrix = -matrix, coordinates = area_centers, node_labels = area_labels, threshold=0.1)\n",
        "\n",
        "  if Connection_Type == 'Excitatory - Inhibitory':\n",
        "      matrix = FLN * (2 * SLN - 1)\n",
        "      plot_matrix_EI(matrix = matrix, coordinates = area_centers, node_labels = area_labels, threshold=0.1)\n",
        "\n",
        "_ = widgets.interact(plot_counterstream, Connection_Type = ['Excitatory', 'Inhibitory', 'Excitatory - Inhibitory'])\n"
      ],
      "metadata": {
        "id": "cSzQZ2MfCgm6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's look now at just a subset of nodes, which will make it easier to understand how the effect of counterstream inhibition relates to the hierarchical organization of the cortex.\n",
        "\n",
        "# @markdown 1. Pay attention to the hierarchical value of the different areas (represented by the color of nodes: brighter = higher hierachical value). Can you find a pattern of E-I connectivity between areas with different hierarchical values?\n",
        "\n",
        "# @markdown 2. Can you tell the directionality of hierarchical interactions from the flow of excitation and inhibition?\n",
        "\n",
        "\n",
        "def plot_EI_areas(visualize_areas = ['V1', 'V2', 'V4', 'MT']):\n",
        "\n",
        "  if len(visualize_areas) > 1:\n",
        "\n",
        "    idxs = [list(area_labels).index(v) for v in visualize_areas]\n",
        "\n",
        "    # Selects only nodes in list\n",
        "    centers_aux = area_centers[np.array(idxs), :]\n",
        "    matrix_aux = (FLN * (2 * SLN - 1))[:, np.array(idxs)][np.array(idxs)]\n",
        "    h_aux = h[np.array(idxs)].squeeze()\n",
        "\n",
        "    plot_matrix_EI(matrix_aux, centers_aux, node_sizes = 2, node_colors = h_aux, vmin = 0, vmax = 1, node_cmap = 'inferno',\n",
        "                  node_labels=visualize_areas, threshold = 1,  node_label_color = 'k')\n",
        "\n",
        "  else:\n",
        "    print('Please select at least 2 areas!')\n",
        "\n",
        "\n",
        "_ = widgets.interact(plot_EI_areas,\n",
        "                     visualize_areas = widgets.SelectMultiple(value = ['V1', 'V2', 'V4', 'MT'],\n",
        "                                                              description = 'Cortical Areas',\n",
        "                                                              options = area_labels))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "m-vDxhUcxSr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Large-scale Model of the Macaque Cortex"
      ],
      "metadata": {
        "id": "wGG5k1rWqGez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the next cell to load data."
      ],
      "metadata": {
        "id": "zEetT0ET6sfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "data = load_and_preprocess_data()\n",
        "area_names = data['area_names']\n",
        "area_lobes = data['area_lobes']\n",
        "area_descriptions = data['area_descriptions']\n",
        "F = data['SLN'] # Supragranular labelled neurons matrix\n",
        "W = data['W'] # FLN based structural connectivity matrix\n",
        "h = data['hier_vals'] # Hierarchy score of each area in the same order as area_names"
      ],
      "metadata": {
        "id": "BSyjlVPg6yuS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the necessary ingredients to implement a large-scale model of the macaque cortex:\n",
        "\n",
        "- local dynamic model\n",
        "- anatomical structural connectivity\n",
        "- hierarchical relationships\n",
        "- counter-stream inhibitory bias\n",
        "\n",
        "In the next sections, we are going to combine these elements and observe the large-scale dynamics.\n"
      ],
      "metadata": {
        "id": "xstHD3u_pkQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Synaptic strength, local and long-range inputs to each node\n",
        "\n",
        "In the large-scale model, each area has a distinct value of incoming synaptic strength, $J_S$, given by:\n",
        "$$\n",
        "J_S(i) = J_{MIN} \\;+\\; (J_{MAX} - J_{MIN}) h_i\n",
        "$$\n",
        "\n",
        "As you've seen in the first section of the tutorial, the bifurcation point of an isolated area is at $J_S$ = 0.4655 nA for our set of parameter values. We set $J_{MAX}$ below that value, implying that all areas in the network are monostable in isolation. In this situation, any sustained activity displayed by the model will be a consequence of a global, cooperative effect due to inter-areal interactions.\n",
        "\n",
        "We compute the incoming synaptic strength (both local and long-range) of a given area as a linear function of the dendritic spine count values observed in anatomical studies, with age-related corrections when necessary. Alternatively, when spine count data is not available for a given area, we will use its position in the anatomical hierarchy, which displays a high correlation with the spine count data, as a proxy for the latter. After this process, the large-scale network will display a gradient of local and long-range recurrent strength, with sensory/association areas showing weak/strong local connectivity, respectively. We denote the local and long-range strength value of a given area $i$ in this gradient as $h_i$, and this value normalized between zero (bottom of the gradient, area V1) and one.\n",
        "\n",
        "Here is how the gradient of J_S looks like for our model:"
      ],
      "metadata": {
        "id": "7wBqw7RypyMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1.1 Gradient of J_s\n",
        "# Plot gradient of J_s across all cortical areas.\n",
        "# Note that the areas on the x-axis are ordered by anatomical hierarchy, but\n",
        "# most J_S values are extracted using spine counts, with anatomical hierarchy\n",
        "# used for areas where we could not obtain this information.\n",
        "WM_params_plot = WorkingMemoryParameters()\n",
        "J_S = WM_params_plot.J_MIN + (WM_params_plot.J_MAX - WM_params_plot.J_MIN) * h\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(area_names, J_S)\n",
        "ax.tick_params(\"x\", rotation=70)\n",
        "ax.set_xlabel('Cortical Area')\n",
        "ax.set_ylabel('Local Synaptic Strenght, J_S')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jQ3y3fi0pxvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total input current $I^x_i$ for each population $‘i’$ - (A, B or C) of a given node $x$ is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "I_A^{x} = J_S S_A^{x} \\;+\\; J_C S_B^{x} \\;+\\; J_{EI} S_C^{x} \\;+\\; I_{ext_A} \\;+\\; I_{\\mathrm{net}}^{A, x} \\;+\\; x_{A}(t)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "I_B^{x} = J_C S_A^{x} \\;+\\; J_S S_B^{x} \\;+\\; J_{EI} S_C^{x} \\;+\\; I_{ext_B} \\;+\\; I_{\\mathrm{net}}^{B, x} \\;+\\; x_{B}(t)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "I_C^{x} = J_{IE} S_A^{x} \\;+\\; J_{IE} S_B^{x} \\;+\\; J_{II} S_C^{x} \\;+\\; I_{ext_C} \\;+\\; I_{\\mathrm{net}}^{C, x} \\;+\\; x_{C}(t)\n",
        "\\end{equation}\n",
        "\n",
        "In these equations, $J_S$, $J_C$ are the self- and cross-coupling between excitatory populations. Likewise, $J_{EI}$ , $J_{IE}$ and $J_{II}$ are the coupling from the inhibitory populations to any of the excitatory ones, the coupling from any of the excitatory populations to the inhibitory one, and the self-coupling strength of the inhibitory population, respectively. The parameters $I_{ext_i}$ with $i$ = A, B, C are background inputs to each population. Finally, the term $I_{\\mathrm{net}}^{i}$ denotes the\n",
        "long-range input coming from other areas in the network,\n",
        "and the term $x_i(t)$ with $i$ = A, B, C represents noise (for details, see https://elifesciences.org/articles/72136).\n",
        "\n",
        "The long-range input to each node $x$ from all other nodes $y$ is described by the following equations:\n",
        "\n",
        "\\begin{equation}\n",
        "I_{\\mathrm{net}}^{A, x}\n",
        "= G \\sum_{y} W^{xy}\\,SLN^{xy}\\,S^y_{A}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "I_{\\mathrm{net}}^{B, x}\n",
        "= G \\sum_{y} W^{xy}\\,SLN^{xy}\\,S^y_{B}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "I_{\\mathrm{net}}^{C, x}\n",
        "= \\frac{\\alpha G}{Z} \\sum_{y} W^{xy}\\,(1 - SLN^{xy})\\,(S^y_{A} + S^y_{B})\n",
        "\\end{equation}\n",
        "\n",
        "where nodes A and B are the excitatory populations, and C is the inhibitory one."
      ],
      "metadata": {
        "id": "j7pWg0PMqSuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that long-range projections are influenced by two controllable parameters: the global coupling, $G$, and the strength of the feedback inhibition, $\\alpha$. Later on in the tutorial, we will observe in more detail how altering the values of these parameters influences the large-scale activity. For now, take a few moments to understand how $G$ and $\\alpha$ influence the long-range connection strengths of the network."
      ],
      "metadata": {
        "id": "lb3V_AnLqYAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1.2 Effect of global coupling and alpha on the balance between long-range excitation and inhibition\n",
        "def plot_G_alpha(G=0.48, alpha=1):\n",
        "    matrix = G * FLN * (SLN - alpha * (1 -SLN))\n",
        "    plot_matrix_EI(matrix = matrix, coordinates = area_centers, node_labels = area_labels, threshold=0.1)\n",
        "_ = widgets.interact(plot_G_alpha,  G=(0.2, 1.8, 0.01), alpha = (0.4, 1.5, 0.1))"
      ],
      "metadata": {
        "id": "ChrGPKhmKRgH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to simulate the large-scale network. Below the activity traces, you will see a figure highlighting on the connectome in yellow which areas exhibit high persistent activity."
      ],
      "metadata": {
        "id": "oADaRIT6qo3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.3 Simple working memory task"
      ],
      "metadata": {
        "id": "298LR88gMfUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simulation\n",
        "random_seed = 123\n",
        "outside_rng = np.random.default_rng(random_seed)\n",
        "# Initial condition and parameters\n",
        "N = len(area_names)\n",
        "Y0 = np.zeros((N, 8))\n",
        "Y0[:, 0:3] = 5 * (1 + np.tanh(outside_rng.normal(0, 2, (N, 3)))) # r values are initialized randomly between 0 and 10\n",
        "WM_params = WorkingMemoryParameters()\n",
        "WM_params.update(G=0.48, J_MAX=0.42, ALPHA=1.0)\n",
        "# Generate the network\n",
        "WM_network = WorkingMemoryNetwork(area_names=area_names, area_lobes=area_lobes, Y0=Y0, W=W, F=F, h=h, params=WM_params)\n",
        "\n",
        "# Run the simulation\n",
        "t_end = 8\n",
        "dt = 0.5e-3\n",
        "I_ext_strength = np.zeros((N, 3))\n",
        "I_ext_strength[0, 0] = 0.3\n",
        "state_history = WM_network.run(t_end=t_end, dt=dt, I_ext_strengths=np.array([I_ext_strength]), ts_ext_start=np.array([4]), ts_ext_end=np.array([4.5]))\n",
        "avg_firing_rates_A = WM_network.avg_firing_rates_last_two_seconds()\n",
        "persistent = [1 if fr > 10 else 0 for fr in avg_firing_rates_A]\n",
        "\n",
        "# Plot the results\n",
        "_ = WM_network.plot_n_areas(xlim=(-4, t_end-4), ylim=(0, 50), title='Simple Working Memory Task')\n",
        "plot_matrix(FLN, area_centers, node_colors = persistent, node_cmap = 'inferno', node_labels=area_labels, node_label_color='k')"
      ],
      "metadata": {
        "id": "7geK5_dMqm7U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Excitation-inhibition Balance"
      ],
      "metadata": {
        "id": "N7t_3B-zkR-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you will explore the effect of the global coupling, $G$, and the relative inhibitory bias, $\\alpha$, on the temporal-activity traces across the nodes of the network. We set the initial conditions to represent the baseline case that is presented in the paper: $G$ = 0.48 and $\\alpha$ = 1. While observing the dynamics, you can explore the following questions:\n",
        "1. By how much can we increase / decrease the global coupling and still observe meaningful dynamics? What happens to stability beyond that threshold?\n",
        "2. Why is a counter‐stream inhibitory bias ($\\alpha$) important? What if you set $\\alpha$ to 0, do you see runaway excitation or pathological suppression?\n",
        "\n",
        "<em>Bonus</em>:\n",
        "Can you think of other methods or mechanisms that would achieve a similar balance between excitation and inhibition?"
      ],
      "metadata": {
        "id": "okCkHZHkrAM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.2.1 Global coupling and feedback inhibition exploration\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def G_alpha_exploration(G=0.48, alpha=1):\n",
        "    # Initial condition and parameters\n",
        "    N = len(area_names)\n",
        "    Y0 = np.zeros((N, 8))\n",
        "    Y0[:, 0:3] = 5 * (1 + np.tanh(outside_rng.normal(0, 2, (N, 3)))) # r values are initialized randomly between 0 and 10\n",
        "    WM_params = WorkingMemoryParameters()\n",
        "\n",
        "    WM_params.update(G=G, ALPHA=alpha, J_MAX=0.42)\n",
        "\n",
        "    WM_network = WorkingMemoryNetwork(area_names=area_names, area_lobes=area_lobes, Y0=Y0, W=W, F=F, h=h, params=WM_params, random_seed=8)\n",
        "    # Run the simulation\n",
        "    t_end = 8\n",
        "    dt = 0.5e-3\n",
        "    I_ext_strength = np.zeros((N, 3))\n",
        "    I_ext_strength[0, 0] = 0.3\n",
        "    state_history = WM_network.run(t_end=t_end, dt=dt, I_ext_strengths=np.array([I_ext_strength]),\n",
        "                                   ts_ext_start=np.array([4]), ts_ext_end=np.array([4.5]))\n",
        "    _ = WM_network.plot_all_areas(xlim=(-4, t_end - 4), ylim=(0, 50))\n",
        "    avg_firing_rates_A = WM_network.avg_firing_rates_last_two_seconds()\n",
        "    persistent = [1 if fr > 10 else 0 for fr in avg_firing_rates_A]\n",
        "    plot_matrix(FLN, area_centers, node_colors = persistent, node_cmap = 'inferno', node_labels=area_labels, node_label_color='k')\n",
        "\n",
        "_ = widgets.interact(G_alpha_exploration, G=(0.2, 0.8, 0.01), alpha = (0.4, 1.5, 0.1))\n"
      ],
      "metadata": {
        "id": "qU12OWc-q86k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Inactivating cortical areas and effects on dynamics\n",
        "\n",
        "Lesions have been an important part of scientific discovery in neuroscience, allowing us to perform causal experiments and explore the role of individual brain regions in various settings. Optogenetics allows experimentalists to temporarily inhibit (or excite) specific parts of the neural tissue, in a reversible manner, making it a very useful technique to investigate the effects that specific brain areas could have on dynamics or performance during cognitive tasks.\n",
        "\n",
        "In this exercise you'll play the role of a virtual experimentalist. By “inactivating” different cortical areas in our large-scale working memory model, you'll discover which nodes are most critical for maintaining a memory trace and why. Think about the following question: if you could silence one brain region at a time, how would you decide which to pick? After thinking about this question for a minute or two, please continue to 'Exploratory study'.\n",
        "\n",
        "<details>\n",
        "<summary>Exploratory study</summary>\n",
        "\n",
        "- **Random lesions:** Turn off 1-5 areas at random. How many does it take before the network fails?\n",
        "- **Hub lesions:** Turn off the areas with the strongest structural connections. Does performance break down faster than with random lesions?\n",
        "- **Hierarchy lesions:** Turn off the top areas in the hierarchy. How does that compare?\n",
        "- **Excitability effects:** Try two different values of the network's global excitation strength (G) or local excitation strength (J_MAX). Does that make the network more or less resistant to lesions?\n",
        "- **Your own idea:** Think of another way to pick areas (for example, shortest path length, clustering) and test it.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "Questions to think about:\n",
        "\n",
        "- Which lesion strategy causes the steepest drop in neural activity across the network?\n",
        "- About how many areas need to be silenced before the network can no longer hold the memory?\n",
        "- What do your results tell you about how this model balances excitation and inhibition?\n",
        "\n",
        "<details>\n",
        "<summary>Hints</summary>\n",
        "\n",
        "- Think about what you learnt in Section 2.\n",
        "- For hub lesions, rank areas by the sum of their incoming and outgoing weights in the connectivity matrix.\n",
        "- For hierarchy lesions, use the hierarchy score provided by the model.\n",
        "- To test excitability, pick one “low” and one “high” value for the main global coupling parameter.\n",
        "</details>"
      ],
      "metadata": {
        "id": "KME3O7xmrk_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To choose which areas to inactivate, run the following code block and select one or multiple areas from the dropdown. You can select more than one area by holding Ctrl + click on all areas you wish to inactivate."
      ],
      "metadata": {
        "id": "hhHf2D1OsJ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select which areas to inactivate\n",
        "inactivated_areas = widgets.SelectMultiple(description='Areas to inactivate:', options=area_names)\n",
        "inactivated_areas"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4ssexBLcsAZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After selecting which areas to inactivate, run the following code block to observe the activity traces across the network. Blue traces represent the case in which inactivations were applied, while grey traces indicate the baseline experiment (no inactivations). You will also see a connectome plot which shows which areas you lesioned (in red), which exhibit high persistent activity (in yellow) and which exhibit low activity (standard grey).\n",
        "\n",
        "Feel free to return to the cell above, select different sets of areas to inactivate and repeat the experiment. Below this code block, you will find a helper function that allows you to plot the mean firing rate during the delay period."
      ],
      "metadata": {
        "id": "kmiA1irRsnNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Working memory simulation with inactivations\n",
        "# Initial condition and parameters\n",
        "N = len(area_names)\n",
        "Y0 = np.zeros((N, 8))\n",
        "Y0[:, 0:3] = 5 * (1 + np.tanh(outside_rng.normal(0, 2, (N, 3))))  # r values are initialized randomly between 0 and 10\n",
        "distributed_WM_params = WorkingMemoryParameters()\n",
        "distributed_WM_params.update(J_MAX=0.26, G=0.48)\n",
        "\n",
        "# Generate the network\n",
        "distributed_WM_network = WorkingMemoryNetwork(area_names=area_names, area_lobes=area_lobes, Y0=Y0, W=W, F=F, h=h, params=distributed_WM_params, random_seed=8)\n",
        "\n",
        "# Prepare the simulation\n",
        "t_end = 15\n",
        "dt = 0.5e-3\n",
        "I_ext_strength = np.zeros((N, 3))\n",
        "I_ext_strength[0, 0] = 0.3\n",
        "\n",
        "# Prepare the visualization\n",
        "target_area_names = ['V1', 'MT', 'LIP', '24c', 'STPi', '9/46d'] # If using plot_n_areas, these areas will be plotted if in rows we change area_names to target_area_names\n",
        "rows = math.ceil(len(area_names) / 6)\n",
        "fig, axes = plt.subplots(rows, 6, figsize=(10, 1.5 * rows), dpi=300)\n",
        "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "axes = axes.flatten() # Flatten for easy iteration\n",
        "\n",
        "# No lesion\n",
        "distributed_WM_network.reset()\n",
        "state_history_no_lesion = distributed_WM_network.run(t_end=t_end, dt=dt, I_ext_strengths=np.array([I_ext_strength]), ts_ext_start=np.array([4]), ts_ext_end=np.array([4.5]))\n",
        "lines1 = distributed_WM_network.plot_all_areas(xlim=(-4, t_end-4), ylim=(0, 50), axes_flat=axes, variables=['r_A'], colors=[[.6, .6, .6]], legend=False)\n",
        "avg_firing_rates_A_no_lesion = distributed_WM_network.avg_firing_rates_last_two_seconds()\n",
        "# Lesion areas\n",
        "distributed_WM_network.reset()\n",
        "state_history_lesion = distributed_WM_network.run(t_end=t_end, dt=dt, I_ext_strengths=np.array([I_ext_strength]), ts_ext_start=np.array([4]), ts_ext_end=np.array([4.5]), lesion_areas=inactivated_areas.value)\n",
        "lines2 = distributed_WM_network.plot_all_areas(xlim=(-4, t_end-4), ylim=(0, 50), axes_flat=axes, variables=['r_A'], colors=[[.1, .6, .8]], legend=False)\n",
        "avg_firing_rates_A_lesion = distributed_WM_network.avg_firing_rates_last_two_seconds()\n",
        "persistent = [10 if fr > 10 else 0 for fr in avg_firing_rates_A_lesion]\n",
        "for lesion_area in inactivated_areas.value:\n",
        "    area_idx = np.where(np.array(area_names) == lesion_area)[0][0]\n",
        "    persistent[area_idx] = 5\n",
        "# Set the legend\n",
        "lines = lines1 + lines2\n",
        "labels1 = [line.get_label()+' (no lesion)' for line in lines1]\n",
        "labels2 = [line.get_label()+f' (silenced {inactivated_areas.value})' for line in lines2]\n",
        "labels = labels1 + labels2\n",
        "for ax_ids in range(5, len(axes), 6):\n",
        "    axes[ax_ids].legend(handles=lines, labels=labels, loc='upper left', fontsize='small', bbox_to_anchor=(1.04, 1))\n",
        "plt.show()\n",
        "\n",
        "plot_matrix(FLN, area_centers, node_colors=persistent, node_cmap='inferno', node_labels=area_labels, node_label_color='k', vmin=0, vmax=10)"
      ],
      "metadata": {
        "id": "1f6X3oHnrpKs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper plot to visualize mean delay-activity across network\n",
        "\n",
        "Run the following code block to visualize the delay-activity firing rate across all areas of the network in baseline vs. inactivation conditions."
      ],
      "metadata": {
        "id": "KTh9C4sitIM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot delay firing rates\n",
        "fig, ax = plt.subplots(figsize=(6, 3), dpi=200)\n",
        "ax.plot(range(len(avg_firing_rates_A_no_lesion)), avg_firing_rates_A_no_lesion, 'o', color=[.6, .6, .6], markersize=5, label='r_A_no_lesion')\n",
        "ax.plot(range(len(avg_firing_rates_A_lesion)), avg_firing_rates_A_lesion, 'o', color=[.1, .6, .8], markersize=5, label=f'r_A_lesioned: {inactivated_areas.value}')\n",
        "ax.set_xticks(np.arange(len(avg_firing_rates_A_no_lesion)), labels=area_names, rotation=70, fontsize=8)\n",
        "ax.set_xlabel(\"Areas sorted by hierarchy (low -> high)\", fontsize=8)\n",
        "ax.set_ylabel(\"Firing Rate (Hz)\", fontsize=8)\n",
        "ax.set_title(f\"Average firing rates in the last 2s of the simulation\", fontsize=10, fontweight='bold')\n",
        "plt.legend(fontsize=7)\n",
        "plt.tight_layout()\n",
        "ax = plt.gca()\n",
        "for spine in ['top','right']:\n",
        "    ax.spines[spine].set_visible(False)\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.grid(linestyle=':', alpha=0.5)\n",
        "ax.tick_params(axis='both', which='major')\n",
        "ax.tick_params(axis='both', which='minor')"
      ],
      "metadata": {
        "id": "UU-doDF7tHSq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}